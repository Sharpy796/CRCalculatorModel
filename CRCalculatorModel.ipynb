{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kbKiDiKvAi9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "pMLIk29tK2bc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning) # TODO: Actually optimize the source of this warning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJKA-xPvDPf"
      },
      "source": [
        "# Import Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpJwQSI5Li6s",
        "outputId": "bb76207a-3a37-4568-d694-9eaec72fcca3"
      },
      "outputs": [],
      "source": [
        "# Set the path to the file you'd like to load\n",
        "file_path = \"aidedd_blocks2.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df_original = kagglehub.dataset_load(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"travistyler/dnd-5e-monster-manual-stats\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "df = df_original.copy()\n",
        "# Create dataframe to keep track of the original values of each categorical data\n",
        "original_categorical_vals = pd.DataFrame()\n",
        "\n",
        "def update_ocv(col, unique):\n",
        "  global original_categorical_vals\n",
        "  original_categorical_vals = pd.concat([original_categorical_vals, pd.DataFrame({col:unique})], axis=1)\n",
        "\n",
        "def __reclassify_categorical__(df, col):\n",
        "  df_copy = df.copy()\n",
        "  unique = df_copy[col].unique()\n",
        "  update_ocv(col, unique)\n",
        "  for i in range(0,len(unique)):\n",
        "    df_copy = df_copy.replace({col: unique[i]}, i)\n",
        "  return df_copy\n",
        "\n",
        "def view_categorical_legend(col):\n",
        "  return original_categorical_vals[col].dropna()\n",
        "\n",
        "def __reclassify_list__(df, col, delimiter):\n",
        "  df_copy = df.copy()\n",
        "  column = df_copy[col]\n",
        "  for i in range(0,len(column)):\n",
        "    num = 0\n",
        "    item = column[i]\n",
        "    vals = item.split(delimiter)\n",
        "    for each in vals:\n",
        "      each = each.lower()\n",
        "      if \"two\" in each: num = num + 2\n",
        "      elif \"three\" in each: num = num + 3\n",
        "      elif \"four\" in each: num = num + 4\n",
        "      elif \"five\" in each: num = num + 5\n",
        "      else: num = num + 1\n",
        "    df_copy.at[i,col] = num\n",
        "  return df_copy\n",
        "\n",
        "# # print(df.dtypes)\n",
        "# TODO: Play with category types??\n",
        "# df_test = df['size'].astype('category')\n",
        "\n",
        "\n",
        "# df = reclassify_categorical(df, 'size')\n",
        "# df = reclassify_categorical(df, 'alignment')\n",
        "# df = reclassify_categorical(df, 'type')\n",
        "df = __reclassify_list__(df, 'languages', \", \")\n",
        "df = __reclassify_list__(df, 'senses', \", \")\n",
        "# print(df['attributes'][2].split(\" | \"))\n",
        "# print(df['actions'][2].split(\" | \"))\n",
        "# print(df['legendary_actions'][2].split(\" | \"))\n",
        "# view_categorical_legend('alignment')\n",
        "\n",
        "# temporary removing of string values so I can work only on num values\n",
        "df = df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "# remove name and source because these don't contribute anything\n",
        "df = df.drop(['name','source'],axis=1)\n",
        "# fix numeric values into floats\n",
        "# for each in df.columns:\n",
        "#   if each not in original_categorical_vals.columns:\n",
        "#     df[each] = df[each].astype(np.float32)\n",
        "#   else:\n",
        "#     # df[each] = df[each].astype(int) # apparently it doesn't like integers\n",
        "#     df[each] = df[each].astype(np.float32)\n",
        "\n",
        "# # categorical_cols = original_categorical_vals.columns\n",
        "# target_col = 'cr'\n",
        "# categorical_cols = ['size','alignment','type']\n",
        "# numerical_cols = df.drop(categorical_cols,axis=1).columns\n",
        "# numerical_cols = df.drop([target_col],axis=1).columns\n",
        "# cat_maps = {}\n",
        "\n",
        "# for col in categorical_cols:\n",
        "#     unique_vals = df[col].unique()\n",
        "#     cat_maps[col] = {val: i for i, val in enumerate(unique_vals)}\n",
        "\n",
        "# for col in categorical_cols:\n",
        "#     df[col] = df[col].map(cat_maps[col]).astype(int)\n",
        "\n",
        "# items = ['numerical','categorical','cr']\n",
        "# data_list = [] # this is the data that we are going to be using\n",
        "\n",
        "# for _, row in df.iterrows():\n",
        "#     data_list.append({\n",
        "#         \"numerical\": row[numerical_cols].astype(float).tolist(),\n",
        "#         \"categorical\": {col: int(row[col]) for col in categorical_cols},\n",
        "#         \"cr\": float(row[target_col])\n",
        "#     })\n",
        "\n",
        "# def show_monster(monster):\n",
        "#   print(f'Numerical:\\t{data_list[monster]['numerical']}')\n",
        "#   print(f'Categorical:\\t{data_list[monster]['categorical']}')\n",
        "#   print(f'CR:\\t\\t{data_list[monster]['cr']}')\n",
        "\n",
        "# show_monster(1)\n",
        "# show_monster(2)\n",
        "# show_monster(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUBv09tBIEL"
      },
      "source": [
        "# Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "zj0dSPtbBHm6"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[231]\u001b[39m\u001b[32m, line 173\u001b[39m\n\u001b[32m    170\u001b[39m mds = MonsterDataset(\u001b[33m\"\u001b[39m\u001b[33maidedd_blocks2.csv\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,transform=transforms.Compose([ToTensor()]))\n\u001b[32m    171\u001b[39m ocv = mds.getocv()\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmds\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# for i, sample in enumerate(mds):\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m#     print(i, sample['numerical'], sample['categorical'])\u001b[39;00m\n\u001b[32m    177\u001b[39m \n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m#     if i == 3:\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;66;03m#         break\u001b[39;00m\n\u001b[32m    182\u001b[39m dataloader = DataLoader(mds, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers=\u001b[32m0\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[231]\u001b[39m\u001b[32m, line 154\u001b[39m, in \u001b[36mMonsterDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    151\u001b[39m sample = {\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m:monster_name,\u001b[33m'\u001b[39m\u001b[33mnumerical\u001b[39m\u001b[33m'\u001b[39m:numerical_data,\u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m:categorical_data,\u001b[33m'\u001b[39m\u001b[33mcr\u001b[39m\u001b[33m'\u001b[39m:target_data}\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     sample = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sample\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chris\\anaconda3\\envs\\aienv\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[39m, in \u001b[36mCompose.__call__\u001b[39m\u001b[34m(self, img)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[32m     94\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m         img = \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[231]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mToTensor.__call__\u001b[39m\u001b[34m(self, sample)\u001b[39m\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample):\n\u001b[32m    164\u001b[39m     numerical_data, categorical_data, target_data = sample[\u001b[33m'\u001b[39m\u001b[33mnumerical\u001b[39m\u001b[33m'\u001b[39m], sample[\u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m], sample[\u001b[33m'\u001b[39m\u001b[33mcr\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33mnumerical\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumerical_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    166\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mcategorical\u001b[39m\u001b[33m'\u001b[39m: {col: torch.tensor(categorical_data[col].values, dtype=torch.long) \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_data},\n\u001b[32m    167\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mcr\u001b[39m\u001b[33m'\u001b[39m: target_data}\n",
            "\u001b[31mTypeError\u001b[39m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
          ]
        }
      ],
      "source": [
        "# Device configuration, this is to check if GPU is available and run on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = len(df.columns) # 48\n",
        "hidden_size = 100 # number of nodes in hidden layer\n",
        "num_classes = 33 # number of classes, 0, 1/4, 1/2, 1-30\n",
        "num_epochs = 2 # number of times we go through the entire dataset\n",
        "batch_size = 100 # number of samples in one forward/backward pass\n",
        "learning_rate = 0.001 # learning rate\n",
        "\n",
        "\n",
        "class MonsterDataset(Dataset):\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.CAT_COLS = ['size','alignment','type','legendary']\n",
        "        self.NONNUMERIC_COLS = ['size','alignment','type','legendary','name','attributes','actions','legendary_actions']\n",
        "        self.__parsecsv__(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    # def __parsecsvUNUSED__(self, csv_file):\n",
        "    #     self.df_original = pd.read_csv(csv_file)\n",
        "    #     self.df = self.df_original.copy()\n",
        "    #     # temporary removing of string values so I can work only on num values\n",
        "    #     self.df = self.df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "    #     # remove source because these don't contribute anything\n",
        "    #     self.df = self.df.drop(['source'],axis=1)\n",
        "    #     target_col = 'cr'\n",
        "    #     categorical_cols = ['size','alignment','type']\n",
        "    #     numerical_cols = self.df.drop(categorical_cols,axis=1).columns\n",
        "    #     numerical_cols = self.df.drop([target_col,'name'],axis=1).columns\n",
        "    #     self.__reclassify_list__('languages', \", \")\n",
        "    #     self.__reclassify_list__('senses', \", \")\n",
        "    #     self.cat_maps = {}\n",
        "\n",
        "    #     for col in categorical_cols:\n",
        "    #         unique_vals = self.df[col].unique()\n",
        "    #         self.cat_maps[col] = {val: i for i, val in enumerate(unique_vals)}\n",
        "\n",
        "    #     for col in categorical_cols:\n",
        "    #         self.df[col] = self.df[col].map(self.cat_maps[col]).astype(int)\n",
        "\n",
        "    #     items = ['numerical','categorical','cr']\n",
        "    #     data_list = [] # this is the data that we are going to be using\n",
        "\n",
        "    #     for _, row in self.df.iterrows():\n",
        "    #         data_list.append({\n",
        "    #             \"numerical\": row[numerical_cols].astype(float).tolist(),\n",
        "    #             \"categorical\": {col: int(row[col]) for col in categorical_cols},\n",
        "    #             \"cr\": float(row[target_col])\n",
        "    #         })\n",
        "        \n",
        "    #     self.data = data_list\n",
        "    \n",
        "    def __parsecsv__(self, csv_file):\n",
        "        self.df_original = pd.read_csv(csv_file)\n",
        "        self.df = self.df_original.copy()\n",
        "        self.original_categorical_vals = pd.DataFrame()\n",
        "        \n",
        "        self.__reclassify_categorical__('size')\n",
        "        self.__reclassify_categorical__('alignment')\n",
        "        self.__reclassify_categorical__('type')\n",
        "        self.__reclassify_categorical__('legendary')\n",
        "        self.__reclassify_list__('languages', \", \")\n",
        "        self.__reclassify_list__('senses', \", \")\n",
        "\n",
        "        # temporary removing of string values so I can work only on num values\n",
        "        self.df = self.df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "        # remove source because these don't contribute anything\n",
        "        self.df = self.df.drop(['source'],axis=1)\n",
        "        \n",
        "        self.__redefine_datatypes__()\n",
        "    \n",
        "    def __update_ocv__(self, col, unique):\n",
        "        self.original_categorical_vals = pd.concat([self.original_categorical_vals, pd.DataFrame({col:unique})], axis=1)\n",
        "\n",
        "    def __redefine_datatypes__(self):\n",
        "        for each in self.df.columns:\n",
        "            if each not in self.NONNUMERIC_COLS:\n",
        "                self.df[each] = pd.to_numeric(self.df[each], errors='coerce').astype(np.float32)\n",
        "            elif each == 'name':\n",
        "                pass\n",
        "            else:\n",
        "                df[each] = df[each].astype(int) # apparently it doesn't like integers\n",
        "                # self.df[each] = self.df[each].astype(np.float32)\n",
        "    \n",
        "    def __reclassify_categorical__(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        unique = df_copy[col].unique()\n",
        "        self.__update_ocv__(col, unique)\n",
        "\n",
        "        df_copy[col] = pd.Categorical(df_copy[col], categories=unique)\n",
        "        df_copy[col+\"_encoded\"] = df_copy[col].cat.codes\n",
        "        # df_copy[col+\"_encoded\"] = df_copy[col].map(self.original_categorical_vals[col])\n",
        "        # for i in range(0,len(unique)):\n",
        "        #   df_copy = df_copy.replace({col: unique[i]}, i)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_list__(self, col, delimiter):\n",
        "        df_copy = self.df.copy()\n",
        "        column = df_copy[col]\n",
        "        for i in range(0,len(column)):\n",
        "            num = 0\n",
        "            item = column[i]\n",
        "            vals = item.split(delimiter)\n",
        "            for each in vals:\n",
        "                each = each.lower()\n",
        "                if \"two\" in each: num = num + 2\n",
        "                elif \"three\" in each: num = num + 3\n",
        "                elif \"four\" in each: num = num + 4\n",
        "                elif \"five\" in each: num = num + 5\n",
        "                else: num = num + 1\n",
        "            df_copy.at[i,col] = num\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    # def __getitemUNUSED__(self, idx):\n",
        "    #     if torch.is_tensor(idx):\n",
        "    #         idx = idx.tolist()\n",
        "        \n",
        "    #     sample = self.data[idx]\n",
        "    #     if self.transform:\n",
        "    #         sample = self.transform(sample)\n",
        "\n",
        "    #     return sample\n",
        "    \n",
        "    def getocv(self):\n",
        "        return self.original_categorical_vals\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        item = self.df.iloc[idx]\n",
        "        \n",
        "        monster_name = item['name']\n",
        "        numerical_data = item.drop(self.CAT_COLS)\n",
        "        numerical_data = numerical_data.drop(['cr','name'])\n",
        "        categorical_data = item[self.CAT_COLS]\n",
        "        target_data = item['cr']\n",
        "        sample = {'name':monster_name,'numerical':numerical_data,'categorical':categorical_data,'cr':target_data}\n",
        "        \n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample\n",
        "    \n",
        "    def getdf(self):\n",
        "        return self.df\n",
        "\n",
        "class ToTensor(object):\n",
        "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
        "    def __call__(self, sample):\n",
        "        numerical_data, categorical_data, target_data = sample['numerical'], sample['categorical'], sample['cr']\n",
        "        return {'numerical': torch.tensor(numerical_data.values, dtype=torch.float32),\n",
        "                'categorical': {col: torch.tensor(categorical_data[col].values, dtype=torch.long) for col in categorical_data},\n",
        "                'cr': target_data}\n",
        "\n",
        "# mds = MonsterDataset(\"aidedd_blocks2.csv\",\"\",transform=transforms.Compose([ToTensor()]))\n",
        "mds = MonsterDataset(\"aidedd_blocks2.csv\",\"\",transform=transforms.Compose([ToTensor()]))\n",
        "ocv = mds.getocv()\n",
        "\n",
        "print(mds.__getitem__(1))\n",
        "\n",
        "# for i, sample in enumerate(mds):\n",
        "#     print(i, sample['numerical'], sample['categorical'])\n",
        "\n",
        "#     if i == 3:\n",
        "#         break\n",
        "\n",
        "\n",
        "dataloader = DataLoader(mds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "\n",
        "# print(mds.__getitem__(2))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = None\n",
        "test_dataset = None\n",
        "train_loader = None\n",
        "test_loader = None\n",
        "\n",
        "ocv\n",
        "# ['medium', 'large', 'huge', 'gargantuan', 'small', 'tiny']\n",
        "# ['tiny', 'small', 'medium', 'large', 'huge', 'gargantuan']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6oUMaolB8ge",
        "outputId": "45483e63-8ca6-4d51-ebc3-55043de025c5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "WWfsBaoGhNoz",
        "outputId": "16cf988b-4935-41ed-b44c-90df8cfe0eac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSZul2aPBMeB"
      },
      "source": [
        "# Test Code Clipboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnk91r0x-uPq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aienv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
