{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kbKiDiKvAi9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pMLIk29tK2bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\chris\\anaconda3\\envs\\pydml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_directml\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import scipy\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning) # TODO: Actually optimize the source of this warning\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUBv09tBIEL"
      },
      "source": [
        "# Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: privateuseone:0\n"
          ]
        }
      ],
      "source": [
        "# Device configuration, this is to check if GPU is available and run on GPU\n",
        "# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "# print(f\"Using {device} device\")\n",
        "# We use torch_directml.device() to grab the first DirectX12 capable GPU.\n",
        "device = torch_directml.device()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zj0dSPtbBHm6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size 6\n",
            "alignment 17\n",
            "type 15\n",
            "legendary 2\n",
            "input size 76\n"
          ]
        }
      ],
      "source": [
        "class MonsterDataset(Dataset):\n",
        "    def __init__(self, csv_file, train, train_val=.9, transform=None, shuffle=True):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.CAT_COLS = ['size','alignment','type','legendary']\n",
        "        self.NONNUMERIC_COLS = ['size','alignment','type','legendary','name','attributes','actions','legendary_actions']\n",
        "        # Add CR mapping\n",
        "        self.CR_TO_IDX = {\n",
        "            0: 0, 0.125: 1, 0.25: 2, 0.5: 3,\n",
        "            1: 4, 2: 5, 3: 6, 4: 7, 5: 8, 6: 9, 7: 10, 8: 11, 9: 12, 10: 13,\n",
        "            11: 14, 12: 15, 13: 16, 14: 17, 15: 18, 16: 19, 17: 20, 18: 21,\n",
        "            19: 22, 20: 23, 21: 24, 22: 25, 23: 26, 24: 27, 25: 28, 26: 29,\n",
        "            27: 30, 28: 31, 29: 32, 30: 33\n",
        "        }\n",
        "        self.__parsecsv__(csv_file)\n",
        "        if shuffle:\n",
        "            self.df = self.df.sample(frac=1,random_state=796).reset_index(drop=True)\n",
        "\n",
        "\n",
        "        if train:\n",
        "            self.df = self.df.iloc[1:int(self.df.shape[0]*train_val)+1].reset_index(drop=True)\n",
        "        else:\n",
        "            self.df = self.df.iloc[int(self.df.shape[0]*train_val):self.df.shape[0]].reset_index(drop=True)\n",
        "\n",
        "        # Create sub dfs of categorized data\n",
        "        df_size = self.create_subdf(\"size\")\n",
        "        df_type = self.create_subdf(\"type\")\n",
        "        df_align = self.create_subdf(\"alignment\")\n",
        "        df_legend = self.create_subdf(\"legendary\")\n",
        "\n",
        "        cols_to_drop = list(df_size.columns) + list(df_type.columns) + \\\n",
        "                       list(df_align.columns) + list(df_legend.columns) + \\\n",
        "                       ['name', 'cr']\n",
        "        # Only drop columns that actually exist to avoid errors\n",
        "        cols_to_drop = [c for c in cols_to_drop if c in self.df.columns]\n",
        "        df_numeric = self.df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "        # Convert everything to Tensors NOW\n",
        "        # .values converts pandas to numpy, which creates tensors much faster\n",
        "        self.names = self.df['name'].values\n",
        "\n",
        "        self.data_size = torch.tensor(df_size.values, dtype=torch.long)\n",
        "        self.data_type = torch.tensor(df_type.values, dtype=torch.long)\n",
        "        self.data_align = torch.tensor(df_align.values, dtype=torch.long)\n",
        "        self.data_legend = torch.tensor(df_legend.values, dtype=torch.long)\n",
        "\n",
        "        # Pre-calculate targets\n",
        "        self.data_numeric = torch.tensor(df_numeric.values, dtype=torch.float32)# Map the CR values to indices using a list comprehension or map\n",
        "        targets = self.df['cr'].map(self.CR_TO_IDX).fillna(0).values \n",
        "        self.data_targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __parsecsv__(self, csv_file):\n",
        "        self.df_original = pd.read_csv(csv_file)\n",
        "        self.df = self.df_original.copy()\n",
        "        self.original_categorical_vals = pd.DataFrame()\n",
        "\n",
        "        self.__reclassify_categorical__('size')\n",
        "        self.__reclassify_categorical__('alignment')\n",
        "        self.__reclassify_categorical__('type')\n",
        "        self.__reclassify_categorical__('legendary')\n",
        "        self.__reclassify_list__('languages', \", \")\n",
        "        self.__reclassify_list__('senses', \", \")\n",
        "\n",
        "        # temporary removing of string values so I can work only on num values\n",
        "        self.df = self.df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "        # remove source because these don't contribute anything\n",
        "        self.df = self.df.drop(['source'],axis=1)\n",
        "        \n",
        "        self.__redefine_datatypes__()\n",
        "\n",
        "        for col in self.CAT_COLS:\n",
        "            self.dummify_cat_values(col)\n",
        "        \n",
        "    def dummify_cat_values(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        if col != 'alignment':\n",
        "            dummies = pd.get_dummies(df_copy[col],prefix=col).astype('float32')\n",
        "            df_copy = pd.concat([df_copy,dummies],axis=1)\n",
        "        else:\n",
        "            alignments = df_copy[col].str.split(\",\").tolist()\n",
        "            flat_align = [col+\"_\"+item for sublist in alignments for item in sublist]\n",
        "            set_align = set(flat_align)\n",
        "            unique_align = list(set_align)\n",
        "            df_copy = df_copy.reindex(df_copy.columns.tolist() + unique_align, axis=1, fill_value=0)\n",
        "            # for each value inside column, update the dummy\n",
        "            for i, row in df_copy.iterrows():\n",
        "                for val in row[col].split(\",\"):\n",
        "                    tag = col+\"_\"+val\n",
        "                    df_copy.loc[i,tag] = 1\n",
        "        df_copy = df_copy.drop([col],axis=1)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __update_ocv__(self, df, col, unique):\n",
        "        self.original_categorical_vals = pd.concat([self.original_categorical_vals, pd.DataFrame({col:unique})], axis=1)\n",
        "\n",
        "    def __redefine_datatypes__(self):\n",
        "        df_copy = self.df.copy()\n",
        "        for each in df_copy.columns:\n",
        "            if each in self.CAT_COLS:\n",
        "                df_copy[each] = df_copy[each].astype('category')\n",
        "            elif each == 'name':\n",
        "                pass\n",
        "            else:\n",
        "                df_copy[each] = pd.to_numeric(df_copy[each], errors='coerce').astype(np.float32)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_categorical__(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        if col == 'type':\n",
        "            for i,each in enumerate(df_copy[col]):\n",
        "                if \"(\" in each:\n",
        "                    df_copy.at[i,col] = each[:(each.find(\"(\")-1)]\n",
        "        elif col == 'alignment':\n",
        "            for i,each in enumerate(df_copy[col]):\n",
        "                if each not in \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\":\n",
        "                    val = \"\"\n",
        "                    if \"any\" in each:\n",
        "                        if \"non\" in each:\n",
        "                            if \"-good\" in each:\n",
        "                                val = \"lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                            elif \"-lawful\" in each:\n",
        "                                val = \"neutral good,chaotic good,neutral,chaotic neutral,neutral evil,chaotic evil\"\n",
        "                        elif \"evil\" in each:\n",
        "                            val = \"lawful evil,neutral evil,chaotic evil\"\n",
        "                        elif \"chaotic\" in each:\n",
        "                            val = \"chaotic good,chaotic neutral,chaotic evil\"\n",
        "                        else:\n",
        "                            val = \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                    elif \"or\" in each:\n",
        "                        if \"neutral good\" in each and \"neutral evil\" in each:\n",
        "                            val = \"neutral good,neutral evil\"\n",
        "                        elif \"chaotic good\" in each and \"neutral evil\" in each:\n",
        "                            val = \"chaotic good,neutral evil\"\n",
        "                    df_copy.at[i,col] = val\n",
        "\n",
        "\n",
        "        unique = df_copy[col].unique()\n",
        "        self.__update_ocv__(df_copy, col, unique)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_list__(self, col, delimiter):\n",
        "        df_copy = self.df.copy()\n",
        "        column = df_copy[col]\n",
        "        for i in range(0,len(column)):\n",
        "            num = 0\n",
        "            item = column[i]\n",
        "            vals = item.split(delimiter)\n",
        "            for each in vals:\n",
        "                each = each.lower()\n",
        "                if \"two\" in each: num = num + 2\n",
        "                elif \"three\" in each: num = num + 3\n",
        "                elif \"four\" in each: num = num + 4\n",
        "                elif \"five\" in each: num = num + 5\n",
        "                else: num = num + 1\n",
        "            df_copy.at[i,col] = num\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def getocv(self):\n",
        "        return self.original_categorical_vals\n",
        "    \n",
        "\n",
        "    def create_subdf(self,substring):\n",
        "        '''\n",
        "        Isolates a whole section of the dataframe by creating a copy\n",
        "        of it and concatenating columns containing the desired substring.\n",
        "        Meant for creating sub-dataframes of categorized one-hot encoded data.\n",
        "        '''\n",
        "        df_copy = self.df.copy()\n",
        "        subdf = pd.DataFrame()\n",
        "        for each in df_copy:\n",
        "            if substring in each:\n",
        "                subdf = pd.concat([subdf,df_copy[each]],axis=1)\n",
        "        return subdf\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        return (\n",
        "            self.names[idx],\n",
        "            self.data_numeric[idx],\n",
        "            self.data_size[idx],\n",
        "            self.data_type[idx],\n",
        "            self.data_align[idx],\n",
        "            self.data_legend[idx],\n",
        "            self.data_targets[idx]\n",
        "        )\n",
        "    \n",
        "    def getdf(self):\n",
        "        return self.df\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 76\n",
        "hidden_size = 100 # number of nodes in hidden layer\n",
        "num_classes = 34 # number of classes, 0, 1/8, 1/4, 1/2, 1-30\n",
        "num_epochs = 100 # number of times we go through the entire dataset\n",
        "batch_size = 32 # number of samples in one forward/backward pass\n",
        "learning_rate = 0.001 # learning rate\n",
        "train_val = .85\n",
        "\n",
        "# Datasets and Dataloaders\n",
        "train_dataset = MonsterDataset(\"aidedd_blocks2.csv\",train=True,train_val=train_val,shuffle=True)\n",
        "test_dataset = MonsterDataset(\"aidedd_blocks2.csv\",train=False,train_val=train_val,shuffle=True)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "# display(train_dataset.getdf())\n",
        "# display(test_dataset.getdf())\n",
        "\n",
        "train_df = train_dataset.getdf()\n",
        "\n",
        "# Apparently this breaks??? Why is it 76 and not 78?\n",
        "# input_size = len(train_df.columns) # 78 including all the expanded categorical data\n",
        "input_size = 76\n",
        "\n",
        "ocv = train_dataset.getocv()\n",
        "for each in ocv:\n",
        "    print(each, len(ocv[each].dropna()))\n",
        "print(\"input size\",input_size)\n",
        "# ocv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6oUMaolB8ge",
        "outputId": "45483e63-8ca6-4d51-ebc3-55043de025c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, None) torch.Size([150, 76])\n",
            "(None,) torch.Size([150])\n",
            "(None, None) torch.Size([200, 150])\n",
            "(None,) torch.Size([200])\n",
            "(None, None) torch.Size([400, 200])\n",
            "(None,) torch.Size([400])\n",
            "(None, None) torch.Size([300, 400])\n",
            "(None,) torch.Size([300])\n",
            "(None, None) torch.Size([200, 300])\n",
            "(None,) torch.Size([200])\n",
            "(None, None) torch.Size([100, 200])\n",
            "(None,) torch.Size([100])\n",
            "(None, None) torch.Size([50, 100])\n",
            "(None,) torch.Size([50])\n",
            "(None, None) torch.Size([34, 50])\n",
            "(None,) torch.Size([34])\n"
          ]
        }
      ],
      "source": [
        "# ReLU seems bad.\n",
        "# Tanh seems okay.\n",
        "# Sigmoid seems best.\n",
        "\n",
        "class CRPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(CRPredictor,self).__init__()\n",
        "        weight1 = 1.5\n",
        "        weight2 = 2\n",
        "        weight3 = 4\n",
        "        weight4 = 3\n",
        "        weight5 = 2\n",
        "        weight6 = 1\n",
        "        weight7 = .5\n",
        "        \n",
        "        self.l1 = nn.Linear(input_size,int(hidden_size*weight1)) # first layer\n",
        "        self.act1 = nn.Sigmoid() # activation function\n",
        "        self.l2 = nn.Linear(int(hidden_size*weight1),int(hidden_size*weight2)) # second layer\n",
        "        self.act2 = nn.Sigmoid() # activation function\n",
        "        self.l3 = nn.Linear(int(hidden_size*weight2),int(hidden_size*weight3)) # third layer\n",
        "        self.act3 = nn.Sigmoid() # activation function\n",
        "        self.l4 = nn.Linear(int(hidden_size*weight3),int(hidden_size*weight4)) # fourth layer\n",
        "        self.act4 = nn.Sigmoid() # activation function\n",
        "        self.l5 = nn.Linear(int(hidden_size*weight4),int(hidden_size*weight5)) # fifth layer\n",
        "        self.act5 = nn.Sigmoid() # activation function\n",
        "        self.l6 = nn.Linear(int(hidden_size*weight5),int(hidden_size*weight6)) # sixth layer\n",
        "        self.act6 = nn.Sigmoid() # activation function\n",
        "        self.l7 = nn.Linear(int(hidden_size*weight6),int(hidden_size*weight7)) # seventh layer\n",
        "        self.act7 = nn.Sigmoid() # activation function\n",
        "        self.l8 = nn.Linear(int(hidden_size*weight7),num_classes) # eighth layer\n",
        "        self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, numeric, cat1,cat2,cat3,cat4):\n",
        "        x = torch.cat([numeric, cat1,cat2,cat3,cat4],dim=1)\n",
        "        x = self.l1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.act3(x)\n",
        "        x = self.l4(x)\n",
        "        x = self.act4(x)\n",
        "        x = self.l5(x)\n",
        "        x = self.act5(x)\n",
        "        x = self.l6(x)\n",
        "        x = self.act6(x)\n",
        "        x = self.l7(x)\n",
        "        x = self.act7(x)\n",
        "        x = self.l8(x)\n",
        "        # x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = CRPredictor(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for param in model.parameters():\n",
        "    print(param.names, param.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "WWfsBaoGhNoz",
        "outputId": "16cf988b-4935-41ed-b44c-90df8cfe0eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Steps: 21\n",
            "epoch 1/100, step 2/21, loss = 3.5499\n",
            "epoch 1/100, step 4/21, loss = 3.5121\n",
            "epoch 1/100, step 6/21, loss = 3.4491\n",
            "epoch 1/100, step 8/21, loss = 3.3687\n",
            "epoch 1/100, step 10/21, loss = 3.3142\n",
            "epoch 1/100, step 12/21, loss = 3.2806\n",
            "epoch 1/100, step 14/21, loss = 3.4168\n",
            "epoch 1/100, step 16/21, loss = 3.2959\n",
            "epoch 1/100, step 18/21, loss = 3.2113\n",
            "epoch 1/100, step 20/21, loss = 3.2134\n",
            "epoch 2/100, step 2/21, loss = 3.3156\n",
            "epoch 2/100, step 4/21, loss = 3.1023\n",
            "epoch 2/100, step 6/21, loss = 3.0701\n",
            "epoch 2/100, step 8/21, loss = 3.2518\n",
            "epoch 2/100, step 10/21, loss = 3.1760\n",
            "epoch 2/100, step 12/21, loss = 3.2692\n",
            "epoch 2/100, step 14/21, loss = 3.1710\n",
            "epoch 2/100, step 16/21, loss = 3.1607\n",
            "epoch 2/100, step 18/21, loss = 3.0510\n",
            "epoch 2/100, step 20/21, loss = 3.0011\n",
            "epoch 3/100, step 2/21, loss = 2.9580\n",
            "epoch 3/100, step 4/21, loss = 3.1991\n",
            "epoch 3/100, step 6/21, loss = 3.0392\n",
            "epoch 3/100, step 8/21, loss = 3.0231\n",
            "epoch 3/100, step 10/21, loss = 3.1563\n",
            "epoch 3/100, step 12/21, loss = 2.9570\n",
            "epoch 3/100, step 14/21, loss = 2.9658\n",
            "epoch 3/100, step 16/21, loss = 3.0830\n",
            "epoch 3/100, step 18/21, loss = 3.2108\n",
            "epoch 3/100, step 20/21, loss = 3.0351\n",
            "epoch 4/100, step 2/21, loss = 3.2194\n",
            "epoch 4/100, step 4/21, loss = 3.0241\n",
            "epoch 4/100, step 6/21, loss = 2.9403\n",
            "epoch 4/100, step 8/21, loss = 3.1651\n",
            "epoch 4/100, step 10/21, loss = 3.0777\n",
            "epoch 4/100, step 12/21, loss = 3.0440\n",
            "epoch 4/100, step 14/21, loss = 3.1405\n",
            "epoch 4/100, step 16/21, loss = 2.9525\n",
            "epoch 4/100, step 18/21, loss = 2.9989\n",
            "epoch 4/100, step 20/21, loss = 3.3473\n",
            "epoch 5/100, step 2/21, loss = 2.9958\n",
            "epoch 5/100, step 4/21, loss = 2.9630\n",
            "epoch 5/100, step 6/21, loss = 2.9708\n",
            "epoch 5/100, step 8/21, loss = 2.8773\n",
            "epoch 5/100, step 10/21, loss = 3.1306\n",
            "epoch 5/100, step 12/21, loss = 3.0449\n",
            "epoch 5/100, step 14/21, loss = 2.9597\n",
            "epoch 5/100, step 16/21, loss = 3.2054\n",
            "epoch 5/100, step 18/21, loss = 3.1992\n",
            "epoch 5/100, step 20/21, loss = 3.1510\n",
            "epoch 6/100, step 2/21, loss = 2.9507\n",
            "epoch 6/100, step 4/21, loss = 3.0639\n",
            "epoch 6/100, step 6/21, loss = 2.9894\n",
            "epoch 6/100, step 8/21, loss = 3.0126\n",
            "epoch 6/100, step 10/21, loss = 3.1780\n",
            "epoch 6/100, step 12/21, loss = 3.0715\n",
            "epoch 6/100, step 14/21, loss = 3.1319\n",
            "epoch 6/100, step 16/21, loss = 2.9152\n",
            "epoch 6/100, step 18/21, loss = 3.0158\n",
            "epoch 6/100, step 20/21, loss = 3.2142\n",
            "epoch 7/100, step 2/21, loss = 2.9942\n",
            "epoch 7/100, step 4/21, loss = 3.0573\n",
            "epoch 7/100, step 6/21, loss = 3.1011\n",
            "epoch 7/100, step 8/21, loss = 3.0742\n",
            "epoch 7/100, step 10/21, loss = 3.1285\n",
            "epoch 7/100, step 12/21, loss = 3.0473\n",
            "epoch 7/100, step 14/21, loss = 3.2422\n",
            "epoch 7/100, step 16/21, loss = 2.9713\n",
            "epoch 7/100, step 18/21, loss = 2.8078\n",
            "epoch 7/100, step 20/21, loss = 3.3581\n",
            "epoch 8/100, step 2/21, loss = 3.0354\n",
            "epoch 8/100, step 4/21, loss = 3.0456\n",
            "epoch 8/100, step 6/21, loss = 3.2198\n",
            "epoch 8/100, step 8/21, loss = 2.9591\n",
            "epoch 8/100, step 10/21, loss = 3.1290\n",
            "epoch 8/100, step 12/21, loss = 2.9139\n",
            "epoch 8/100, step 14/21, loss = 3.1053\n",
            "epoch 8/100, step 16/21, loss = 3.1074\n",
            "epoch 8/100, step 18/21, loss = 2.9369\n",
            "epoch 8/100, step 20/21, loss = 3.0376\n",
            "epoch 9/100, step 2/21, loss = 3.0266\n",
            "epoch 9/100, step 4/21, loss = 3.0047\n",
            "epoch 9/100, step 6/21, loss = 3.1382\n",
            "epoch 9/100, step 8/21, loss = 2.9418\n",
            "epoch 9/100, step 10/21, loss = 3.0461\n",
            "epoch 9/100, step 12/21, loss = 3.1976\n",
            "epoch 9/100, step 14/21, loss = 3.2222\n",
            "epoch 9/100, step 16/21, loss = 3.1055\n",
            "epoch 9/100, step 18/21, loss = 2.9793\n",
            "epoch 9/100, step 20/21, loss = 3.1312\n",
            "epoch 10/100, step 2/21, loss = 2.9360\n",
            "epoch 10/100, step 4/21, loss = 2.9132\n",
            "epoch 10/100, step 6/21, loss = 2.8550\n",
            "epoch 10/100, step 8/21, loss = 3.0828\n",
            "epoch 10/100, step 10/21, loss = 3.1222\n",
            "epoch 10/100, step 12/21, loss = 2.9012\n",
            "epoch 10/100, step 14/21, loss = 3.1781\n",
            "epoch 10/100, step 16/21, loss = 2.9896\n",
            "epoch 10/100, step 18/21, loss = 3.0056\n",
            "epoch 10/100, step 20/21, loss = 2.9184\n",
            "epoch 11/100, step 2/21, loss = 3.0300\n",
            "epoch 11/100, step 4/21, loss = 2.9561\n",
            "epoch 11/100, step 6/21, loss = 3.4320\n",
            "epoch 11/100, step 8/21, loss = 3.0696\n",
            "epoch 11/100, step 10/21, loss = 3.1523\n",
            "epoch 11/100, step 12/21, loss = 3.2164\n",
            "epoch 11/100, step 14/21, loss = 2.9606\n",
            "epoch 11/100, step 16/21, loss = 3.0113\n",
            "epoch 11/100, step 18/21, loss = 2.9798\n",
            "epoch 11/100, step 20/21, loss = 3.0092\n",
            "epoch 12/100, step 2/21, loss = 3.1911\n",
            "epoch 12/100, step 4/21, loss = 3.0382\n",
            "epoch 12/100, step 6/21, loss = 2.9737\n",
            "epoch 12/100, step 8/21, loss = 3.2726\n",
            "epoch 12/100, step 10/21, loss = 2.8870\n",
            "epoch 12/100, step 12/21, loss = 3.0117\n",
            "epoch 12/100, step 14/21, loss = 2.9108\n",
            "epoch 12/100, step 16/21, loss = 3.0823\n",
            "epoch 12/100, step 18/21, loss = 2.9402\n",
            "epoch 12/100, step 20/21, loss = 2.9858\n",
            "epoch 13/100, step 2/21, loss = 3.0147\n",
            "epoch 13/100, step 4/21, loss = 3.2972\n",
            "epoch 13/100, step 6/21, loss = 2.8591\n",
            "epoch 13/100, step 8/21, loss = 3.0563\n",
            "epoch 13/100, step 10/21, loss = 3.2589\n",
            "epoch 13/100, step 12/21, loss = 2.9461\n",
            "epoch 13/100, step 14/21, loss = 3.0633\n",
            "epoch 13/100, step 16/21, loss = 3.0640\n",
            "epoch 13/100, step 18/21, loss = 3.1205\n",
            "epoch 13/100, step 20/21, loss = 2.9661\n",
            "epoch 14/100, step 2/21, loss = 2.9890\n",
            "epoch 14/100, step 4/21, loss = 2.8647\n",
            "epoch 14/100, step 6/21, loss = 3.1956\n",
            "epoch 14/100, step 8/21, loss = 3.1557\n",
            "epoch 14/100, step 10/21, loss = 3.1382\n",
            "epoch 14/100, step 12/21, loss = 3.0305\n",
            "epoch 14/100, step 14/21, loss = 3.0472\n",
            "epoch 14/100, step 16/21, loss = 3.0445\n",
            "epoch 14/100, step 18/21, loss = 3.0040\n",
            "epoch 14/100, step 20/21, loss = 3.0053\n",
            "epoch 15/100, step 2/21, loss = 2.8336\n",
            "epoch 15/100, step 4/21, loss = 2.8220\n",
            "epoch 15/100, step 6/21, loss = 3.0555\n",
            "epoch 15/100, step 8/21, loss = 3.0093\n",
            "epoch 15/100, step 10/21, loss = 2.9925\n",
            "epoch 15/100, step 12/21, loss = 3.4190\n",
            "epoch 15/100, step 14/21, loss = 3.2041\n",
            "epoch 15/100, step 16/21, loss = 2.9721\n",
            "epoch 15/100, step 18/21, loss = 2.9381\n",
            "epoch 15/100, step 20/21, loss = 3.0461\n",
            "epoch 16/100, step 2/21, loss = 3.0286\n",
            "epoch 16/100, step 4/21, loss = 3.0680\n",
            "epoch 16/100, step 6/21, loss = 3.0096\n",
            "epoch 16/100, step 8/21, loss = 2.8505\n",
            "epoch 16/100, step 10/21, loss = 2.9442\n",
            "epoch 16/100, step 12/21, loss = 2.9168\n",
            "epoch 16/100, step 14/21, loss = 2.8795\n",
            "epoch 16/100, step 16/21, loss = 2.7865\n",
            "epoch 16/100, step 18/21, loss = 2.9937\n",
            "epoch 16/100, step 20/21, loss = 2.9720\n",
            "epoch 17/100, step 2/21, loss = 2.9066\n",
            "epoch 17/100, step 4/21, loss = 2.8323\n",
            "epoch 17/100, step 6/21, loss = 2.7635\n",
            "epoch 17/100, step 8/21, loss = 2.7867\n",
            "epoch 17/100, step 10/21, loss = 2.7553\n",
            "epoch 17/100, step 12/21, loss = 2.7893\n",
            "epoch 17/100, step 14/21, loss = 2.9047\n",
            "epoch 17/100, step 16/21, loss = 2.9848\n",
            "epoch 17/100, step 18/21, loss = 2.6716\n",
            "epoch 17/100, step 20/21, loss = 2.7831\n",
            "epoch 18/100, step 2/21, loss = 2.8332\n",
            "epoch 18/100, step 4/21, loss = 2.8047\n",
            "epoch 18/100, step 6/21, loss = 2.8252\n",
            "epoch 18/100, step 8/21, loss = 2.7882\n",
            "epoch 18/100, step 10/21, loss = 2.6762\n",
            "epoch 18/100, step 12/21, loss = 2.8837\n",
            "epoch 18/100, step 14/21, loss = 2.6002\n",
            "epoch 18/100, step 16/21, loss = 2.8309\n",
            "epoch 18/100, step 18/21, loss = 2.7995\n",
            "epoch 18/100, step 20/21, loss = 2.5434\n",
            "epoch 19/100, step 2/21, loss = 2.6178\n",
            "epoch 19/100, step 4/21, loss = 2.7842\n",
            "epoch 19/100, step 6/21, loss = 2.6666\n",
            "epoch 19/100, step 8/21, loss = 2.7101\n",
            "epoch 19/100, step 10/21, loss = 2.9264\n",
            "epoch 19/100, step 12/21, loss = 2.6753\n",
            "epoch 19/100, step 14/21, loss = 2.7346\n",
            "epoch 19/100, step 16/21, loss = 2.8096\n",
            "epoch 19/100, step 18/21, loss = 2.6730\n",
            "epoch 19/100, step 20/21, loss = 2.4805\n",
            "epoch 20/100, step 2/21, loss = 2.5103\n",
            "epoch 20/100, step 4/21, loss = 2.7236\n",
            "epoch 20/100, step 6/21, loss = 2.9219\n",
            "epoch 20/100, step 8/21, loss = 2.6481\n",
            "epoch 20/100, step 10/21, loss = 2.8627\n",
            "epoch 20/100, step 12/21, loss = 2.6326\n",
            "epoch 20/100, step 14/21, loss = 2.6007\n",
            "epoch 20/100, step 16/21, loss = 2.7621\n",
            "epoch 20/100, step 18/21, loss = 2.4912\n",
            "epoch 20/100, step 20/21, loss = 2.7414\n",
            "epoch 21/100, step 2/21, loss = 2.5834\n",
            "epoch 21/100, step 4/21, loss = 2.7782\n",
            "epoch 21/100, step 6/21, loss = 2.7255\n",
            "epoch 21/100, step 8/21, loss = 2.8639\n",
            "epoch 21/100, step 10/21, loss = 2.7310\n",
            "epoch 21/100, step 12/21, loss = 2.5416\n",
            "epoch 21/100, step 14/21, loss = 2.5289\n",
            "epoch 21/100, step 16/21, loss = 2.7272\n",
            "epoch 21/100, step 18/21, loss = 2.6935\n",
            "epoch 21/100, step 20/21, loss = 2.7618\n",
            "epoch 22/100, step 2/21, loss = 2.6241\n",
            "epoch 22/100, step 4/21, loss = 2.4640\n",
            "epoch 22/100, step 6/21, loss = 2.5489\n",
            "epoch 22/100, step 8/21, loss = 2.6884\n",
            "epoch 22/100, step 10/21, loss = 2.6050\n",
            "epoch 22/100, step 12/21, loss = 2.6051\n",
            "epoch 22/100, step 14/21, loss = 2.7576\n",
            "epoch 22/100, step 16/21, loss = 2.5827\n",
            "epoch 22/100, step 18/21, loss = 2.6608\n",
            "epoch 22/100, step 20/21, loss = 2.5089\n",
            "epoch 23/100, step 2/21, loss = 2.7448\n",
            "epoch 23/100, step 4/21, loss = 2.5983\n",
            "epoch 23/100, step 6/21, loss = 2.5412\n",
            "epoch 23/100, step 8/21, loss = 2.5326\n",
            "epoch 23/100, step 10/21, loss = 2.6166\n",
            "epoch 23/100, step 12/21, loss = 2.5241\n",
            "epoch 23/100, step 14/21, loss = 2.8550\n",
            "epoch 23/100, step 16/21, loss = 2.6607\n",
            "epoch 23/100, step 18/21, loss = 2.7546\n",
            "epoch 23/100, step 20/21, loss = 2.7635\n",
            "epoch 24/100, step 2/21, loss = 2.3584\n",
            "epoch 24/100, step 4/21, loss = 2.8111\n",
            "epoch 24/100, step 6/21, loss = 2.7863\n",
            "epoch 24/100, step 8/21, loss = 2.5097\n",
            "epoch 24/100, step 10/21, loss = 2.3731\n",
            "epoch 24/100, step 12/21, loss = 2.6248\n",
            "epoch 24/100, step 14/21, loss = 2.6633\n",
            "epoch 24/100, step 16/21, loss = 2.5801\n",
            "epoch 24/100, step 18/21, loss = 2.5788\n",
            "epoch 24/100, step 20/21, loss = 2.6956\n",
            "epoch 25/100, step 2/21, loss = 2.9274\n",
            "epoch 25/100, step 4/21, loss = 2.6070\n",
            "epoch 25/100, step 6/21, loss = 2.6417\n",
            "epoch 25/100, step 8/21, loss = 2.8742\n",
            "epoch 25/100, step 10/21, loss = 2.4826\n",
            "epoch 25/100, step 12/21, loss = 2.6227\n",
            "epoch 25/100, step 14/21, loss = 2.5452\n",
            "epoch 25/100, step 16/21, loss = 2.5838\n",
            "epoch 25/100, step 18/21, loss = 2.5133\n",
            "epoch 25/100, step 20/21, loss = 2.6064\n",
            "epoch 26/100, step 2/21, loss = 2.5907\n",
            "epoch 26/100, step 4/21, loss = 2.6157\n",
            "epoch 26/100, step 6/21, loss = 2.6578\n",
            "epoch 26/100, step 8/21, loss = 2.5754\n",
            "epoch 26/100, step 10/21, loss = 2.6509\n",
            "epoch 26/100, step 12/21, loss = 2.4428\n",
            "epoch 26/100, step 14/21, loss = 2.6657\n",
            "epoch 26/100, step 16/21, loss = 2.4971\n",
            "epoch 26/100, step 18/21, loss = 2.9123\n",
            "epoch 26/100, step 20/21, loss = 2.5990\n",
            "epoch 27/100, step 2/21, loss = 2.4820\n",
            "epoch 27/100, step 4/21, loss = 2.4316\n",
            "epoch 27/100, step 6/21, loss = 2.4792\n",
            "epoch 27/100, step 8/21, loss = 2.7604\n",
            "epoch 27/100, step 10/21, loss = 2.5295\n",
            "epoch 27/100, step 12/21, loss = 2.5643\n",
            "epoch 27/100, step 14/21, loss = 2.3813\n",
            "epoch 27/100, step 16/21, loss = 2.4697\n",
            "epoch 27/100, step 18/21, loss = 2.6016\n",
            "epoch 27/100, step 20/21, loss = 2.5828\n",
            "epoch 28/100, step 2/21, loss = 2.5800\n",
            "epoch 28/100, step 4/21, loss = 2.7435\n",
            "epoch 28/100, step 6/21, loss = 2.4989\n",
            "epoch 28/100, step 8/21, loss = 2.5576\n",
            "epoch 28/100, step 10/21, loss = 2.4055\n",
            "epoch 28/100, step 12/21, loss = 2.6565\n",
            "epoch 28/100, step 14/21, loss = 2.8845\n",
            "epoch 28/100, step 16/21, loss = 2.4456\n",
            "epoch 28/100, step 18/21, loss = 2.5246\n",
            "epoch 28/100, step 20/21, loss = 2.4724\n",
            "epoch 29/100, step 2/21, loss = 2.5756\n",
            "epoch 29/100, step 4/21, loss = 2.6241\n",
            "epoch 29/100, step 6/21, loss = 2.5001\n",
            "epoch 29/100, step 8/21, loss = 2.4167\n",
            "epoch 29/100, step 10/21, loss = 2.5290\n",
            "epoch 29/100, step 12/21, loss = 2.7930\n",
            "epoch 29/100, step 14/21, loss = 2.5533\n",
            "epoch 29/100, step 16/21, loss = 2.3680\n",
            "epoch 29/100, step 18/21, loss = 2.3141\n",
            "epoch 29/100, step 20/21, loss = 2.7016\n",
            "epoch 30/100, step 2/21, loss = 2.6199\n",
            "epoch 30/100, step 4/21, loss = 2.6327\n",
            "epoch 30/100, step 6/21, loss = 2.7102\n",
            "epoch 30/100, step 8/21, loss = 2.3442\n",
            "epoch 30/100, step 10/21, loss = 2.3461\n",
            "epoch 30/100, step 12/21, loss = 2.4137\n",
            "epoch 30/100, step 14/21, loss = 2.5413\n",
            "epoch 30/100, step 16/21, loss = 2.6349\n",
            "epoch 30/100, step 18/21, loss = 2.4914\n",
            "epoch 30/100, step 20/21, loss = 2.6132\n",
            "epoch 31/100, step 2/21, loss = 2.7325\n",
            "epoch 31/100, step 4/21, loss = 2.3637\n",
            "epoch 31/100, step 6/21, loss = 2.5915\n",
            "epoch 31/100, step 8/21, loss = 2.4696\n",
            "epoch 31/100, step 10/21, loss = 2.6865\n",
            "epoch 31/100, step 12/21, loss = 2.5947\n",
            "epoch 31/100, step 14/21, loss = 2.4872\n",
            "epoch 31/100, step 16/21, loss = 2.6224\n",
            "epoch 31/100, step 18/21, loss = 2.5679\n",
            "epoch 31/100, step 20/21, loss = 2.5088\n",
            "epoch 32/100, step 2/21, loss = 2.6925\n",
            "epoch 32/100, step 4/21, loss = 2.7472\n",
            "epoch 32/100, step 6/21, loss = 2.5545\n",
            "epoch 32/100, step 8/21, loss = 2.5906\n",
            "epoch 32/100, step 10/21, loss = 2.4827\n",
            "epoch 32/100, step 12/21, loss = 2.5044\n",
            "epoch 32/100, step 14/21, loss = 2.3974\n",
            "epoch 32/100, step 16/21, loss = 2.8796\n",
            "epoch 32/100, step 18/21, loss = 2.6836\n",
            "epoch 32/100, step 20/21, loss = 2.6053\n",
            "epoch 33/100, step 2/21, loss = 2.4151\n",
            "epoch 33/100, step 4/21, loss = 2.8629\n",
            "epoch 33/100, step 6/21, loss = 2.4833\n",
            "epoch 33/100, step 8/21, loss = 2.4506\n",
            "epoch 33/100, step 10/21, loss = 2.2959\n",
            "epoch 33/100, step 12/21, loss = 2.7286\n",
            "epoch 33/100, step 14/21, loss = 2.3807\n",
            "epoch 33/100, step 16/21, loss = 2.5606\n",
            "epoch 33/100, step 18/21, loss = 2.5833\n",
            "epoch 33/100, step 20/21, loss = 2.3384\n",
            "epoch 34/100, step 2/21, loss = 2.4596\n",
            "epoch 34/100, step 4/21, loss = 2.6009\n",
            "epoch 34/100, step 6/21, loss = 2.6787\n",
            "epoch 34/100, step 8/21, loss = 2.6989\n",
            "epoch 34/100, step 10/21, loss = 2.3128\n",
            "epoch 34/100, step 12/21, loss = 2.5874\n",
            "epoch 34/100, step 14/21, loss = 2.5283\n",
            "epoch 34/100, step 16/21, loss = 2.5172\n",
            "epoch 34/100, step 18/21, loss = 2.4452\n",
            "epoch 34/100, step 20/21, loss = 2.5684\n",
            "epoch 35/100, step 2/21, loss = 2.5459\n",
            "epoch 35/100, step 4/21, loss = 2.7578\n",
            "epoch 35/100, step 6/21, loss = 2.4904\n",
            "epoch 35/100, step 8/21, loss = 2.3766\n",
            "epoch 35/100, step 10/21, loss = 2.3767\n",
            "epoch 35/100, step 12/21, loss = 2.5399\n",
            "epoch 35/100, step 14/21, loss = 2.6186\n",
            "epoch 35/100, step 16/21, loss = 2.4303\n",
            "epoch 35/100, step 18/21, loss = 2.4641\n",
            "epoch 35/100, step 20/21, loss = 2.5168\n",
            "epoch 36/100, step 2/21, loss = 2.5335\n",
            "epoch 36/100, step 4/21, loss = 2.3166\n",
            "epoch 36/100, step 6/21, loss = 2.6316\n",
            "epoch 36/100, step 8/21, loss = 2.3336\n",
            "epoch 36/100, step 10/21, loss = 2.6482\n",
            "epoch 36/100, step 12/21, loss = 2.3125\n",
            "epoch 36/100, step 14/21, loss = 2.7136\n",
            "epoch 36/100, step 16/21, loss = 2.3508\n",
            "epoch 36/100, step 18/21, loss = 2.3354\n",
            "epoch 36/100, step 20/21, loss = 2.4304\n",
            "epoch 37/100, step 2/21, loss = 2.5135\n",
            "epoch 37/100, step 4/21, loss = 2.5314\n",
            "epoch 37/100, step 6/21, loss = 2.3721\n",
            "epoch 37/100, step 8/21, loss = 2.5282\n",
            "epoch 37/100, step 10/21, loss = 2.3934\n",
            "epoch 37/100, step 12/21, loss = 2.4906\n",
            "epoch 37/100, step 14/21, loss = 2.3608\n",
            "epoch 37/100, step 16/21, loss = 2.6238\n",
            "epoch 37/100, step 18/21, loss = 2.5037\n",
            "epoch 37/100, step 20/21, loss = 2.3281\n",
            "epoch 38/100, step 2/21, loss = 2.4542\n",
            "epoch 38/100, step 4/21, loss = 2.3413\n",
            "epoch 38/100, step 6/21, loss = 2.2441\n",
            "epoch 38/100, step 8/21, loss = 2.6292\n",
            "epoch 38/100, step 10/21, loss = 2.4567\n",
            "epoch 38/100, step 12/21, loss = 2.4164\n",
            "epoch 38/100, step 14/21, loss = 2.5437\n",
            "epoch 38/100, step 16/21, loss = 2.4642\n",
            "epoch 38/100, step 18/21, loss = 2.6411\n",
            "epoch 38/100, step 20/21, loss = 2.3402\n",
            "epoch 39/100, step 2/21, loss = 2.3991\n",
            "epoch 39/100, step 4/21, loss = 2.4190\n",
            "epoch 39/100, step 6/21, loss = 2.4554\n",
            "epoch 39/100, step 8/21, loss = 2.4459\n",
            "epoch 39/100, step 10/21, loss = 2.4161\n",
            "epoch 39/100, step 12/21, loss = 2.3907\n",
            "epoch 39/100, step 14/21, loss = 2.3224\n",
            "epoch 39/100, step 16/21, loss = 2.5075\n",
            "epoch 39/100, step 18/21, loss = 2.4511\n",
            "epoch 39/100, step 20/21, loss = 2.6356\n",
            "epoch 40/100, step 2/21, loss = 2.1169\n",
            "epoch 40/100, step 4/21, loss = 2.6573\n",
            "epoch 40/100, step 6/21, loss = 2.5524\n",
            "epoch 40/100, step 8/21, loss = 2.3733\n",
            "epoch 40/100, step 10/21, loss = 2.1988\n",
            "epoch 40/100, step 12/21, loss = 2.5199\n",
            "epoch 40/100, step 14/21, loss = 2.4726\n",
            "epoch 40/100, step 16/21, loss = 2.4795\n",
            "epoch 40/100, step 18/21, loss = 2.2983\n",
            "epoch 40/100, step 20/21, loss = 2.2866\n",
            "epoch 41/100, step 2/21, loss = 2.4550\n",
            "epoch 41/100, step 4/21, loss = 2.1991\n",
            "epoch 41/100, step 6/21, loss = 2.2291\n",
            "epoch 41/100, step 8/21, loss = 2.4354\n",
            "epoch 41/100, step 10/21, loss = 2.3545\n",
            "epoch 41/100, step 12/21, loss = 2.3894\n",
            "epoch 41/100, step 14/21, loss = 2.3461\n",
            "epoch 41/100, step 16/21, loss = 2.4262\n",
            "epoch 41/100, step 18/21, loss = 2.4342\n",
            "epoch 41/100, step 20/21, loss = 2.3815\n",
            "epoch 42/100, step 2/21, loss = 2.4420\n",
            "epoch 42/100, step 4/21, loss = 2.3234\n",
            "epoch 42/100, step 6/21, loss = 2.5971\n",
            "epoch 42/100, step 8/21, loss = 2.6701\n",
            "epoch 42/100, step 10/21, loss = 2.3660\n",
            "epoch 42/100, step 12/21, loss = 2.6453\n",
            "epoch 42/100, step 14/21, loss = 2.1656\n",
            "epoch 42/100, step 16/21, loss = 2.1614\n",
            "epoch 42/100, step 18/21, loss = 2.5006\n",
            "epoch 42/100, step 20/21, loss = 2.2154\n",
            "epoch 43/100, step 2/21, loss = 2.2629\n",
            "epoch 43/100, step 4/21, loss = 2.5971\n",
            "epoch 43/100, step 6/21, loss = 2.3318\n",
            "epoch 43/100, step 8/21, loss = 2.2569\n",
            "epoch 43/100, step 10/21, loss = 2.3724\n",
            "epoch 43/100, step 12/21, loss = 2.2825\n",
            "epoch 43/100, step 14/21, loss = 2.2174\n",
            "epoch 43/100, step 16/21, loss = 2.3947\n",
            "epoch 43/100, step 18/21, loss = 2.2590\n",
            "epoch 43/100, step 20/21, loss = 2.2178\n",
            "epoch 44/100, step 2/21, loss = 2.1444\n",
            "epoch 44/100, step 4/21, loss = 2.3019\n",
            "epoch 44/100, step 6/21, loss = 2.3658\n",
            "epoch 44/100, step 8/21, loss = 2.1923\n",
            "epoch 44/100, step 10/21, loss = 2.2149\n",
            "epoch 44/100, step 12/21, loss = 2.2832\n",
            "epoch 44/100, step 14/21, loss = 2.1679\n",
            "epoch 44/100, step 16/21, loss = 2.3998\n",
            "epoch 44/100, step 18/21, loss = 2.3715\n",
            "epoch 44/100, step 20/21, loss = 2.1975\n",
            "epoch 45/100, step 2/21, loss = 2.6134\n",
            "epoch 45/100, step 4/21, loss = 2.3006\n",
            "epoch 45/100, step 6/21, loss = 2.2060\n",
            "epoch 45/100, step 8/21, loss = 2.2678\n",
            "epoch 45/100, step 10/21, loss = 2.2344\n",
            "epoch 45/100, step 12/21, loss = 2.1779\n",
            "epoch 45/100, step 14/21, loss = 2.1171\n",
            "epoch 45/100, step 16/21, loss = 2.2745\n",
            "epoch 45/100, step 18/21, loss = 2.2750\n",
            "epoch 45/100, step 20/21, loss = 2.4908\n",
            "epoch 46/100, step 2/21, loss = 2.2654\n",
            "epoch 46/100, step 4/21, loss = 2.2730\n",
            "epoch 46/100, step 6/21, loss = 2.2556\n",
            "epoch 46/100, step 8/21, loss = 2.4304\n",
            "epoch 46/100, step 10/21, loss = 2.3545\n",
            "epoch 46/100, step 12/21, loss = 2.2680\n",
            "epoch 46/100, step 14/21, loss = 2.3811\n",
            "epoch 46/100, step 16/21, loss = 2.1904\n",
            "epoch 46/100, step 18/21, loss = 2.1209\n",
            "epoch 46/100, step 20/21, loss = 2.2327\n",
            "epoch 47/100, step 2/21, loss = 2.1875\n",
            "epoch 47/100, step 4/21, loss = 2.3758\n",
            "epoch 47/100, step 6/21, loss = 2.2561\n",
            "epoch 47/100, step 8/21, loss = 2.2223\n",
            "epoch 47/100, step 10/21, loss = 2.1834\n",
            "epoch 47/100, step 12/21, loss = 2.2486\n",
            "epoch 47/100, step 14/21, loss = 2.1035\n",
            "epoch 47/100, step 16/21, loss = 2.1721\n",
            "epoch 47/100, step 18/21, loss = 2.4152\n",
            "epoch 47/100, step 20/21, loss = 2.1602\n",
            "epoch 48/100, step 2/21, loss = 2.1512\n",
            "epoch 48/100, step 4/21, loss = 2.3465\n",
            "epoch 48/100, step 6/21, loss = 2.3368\n",
            "epoch 48/100, step 8/21, loss = 2.3380\n",
            "epoch 48/100, step 10/21, loss = 2.1671\n",
            "epoch 48/100, step 12/21, loss = 2.3914\n",
            "epoch 48/100, step 14/21, loss = 2.1673\n",
            "epoch 48/100, step 16/21, loss = 2.2182\n",
            "epoch 48/100, step 18/21, loss = 1.9625\n",
            "epoch 48/100, step 20/21, loss = 2.1796\n",
            "epoch 49/100, step 2/21, loss = 2.3344\n",
            "epoch 49/100, step 4/21, loss = 2.2469\n",
            "epoch 49/100, step 6/21, loss = 2.4599\n",
            "epoch 49/100, step 8/21, loss = 2.2059\n",
            "epoch 49/100, step 10/21, loss = 2.2597\n",
            "epoch 49/100, step 12/21, loss = 2.2233\n",
            "epoch 49/100, step 14/21, loss = 2.0824\n",
            "epoch 49/100, step 16/21, loss = 2.2516\n",
            "epoch 49/100, step 18/21, loss = 2.0210\n",
            "epoch 49/100, step 20/21, loss = 2.0682\n",
            "epoch 50/100, step 2/21, loss = 2.3273\n",
            "epoch 50/100, step 4/21, loss = 2.3631\n",
            "epoch 50/100, step 6/21, loss = 2.2901\n",
            "epoch 50/100, step 8/21, loss = 2.0488\n",
            "epoch 50/100, step 10/21, loss = 2.0373\n",
            "epoch 50/100, step 12/21, loss = 2.1878\n",
            "epoch 50/100, step 14/21, loss = 1.8654\n",
            "epoch 50/100, step 16/21, loss = 2.4267\n",
            "epoch 50/100, step 18/21, loss = 2.0480\n",
            "epoch 50/100, step 20/21, loss = 2.3191\n",
            "epoch 51/100, step 2/21, loss = 2.0504\n",
            "epoch 51/100, step 4/21, loss = 2.1129\n",
            "epoch 51/100, step 6/21, loss = 2.1959\n",
            "epoch 51/100, step 8/21, loss = 2.3922\n",
            "epoch 51/100, step 10/21, loss = 2.0834\n",
            "epoch 51/100, step 12/21, loss = 2.0667\n",
            "epoch 51/100, step 14/21, loss = 2.0706\n",
            "epoch 51/100, step 16/21, loss = 2.0254\n",
            "epoch 51/100, step 18/21, loss = 2.0304\n",
            "epoch 51/100, step 20/21, loss = 2.1283\n",
            "epoch 52/100, step 2/21, loss = 1.9285\n",
            "epoch 52/100, step 4/21, loss = 2.3877\n",
            "epoch 52/100, step 6/21, loss = 2.0999\n",
            "epoch 52/100, step 8/21, loss = 2.1974\n",
            "epoch 52/100, step 10/21, loss = 2.0510\n",
            "epoch 52/100, step 12/21, loss = 1.8841\n",
            "epoch 52/100, step 14/21, loss = 2.2657\n",
            "epoch 52/100, step 16/21, loss = 2.1011\n",
            "epoch 52/100, step 18/21, loss = 1.8480\n",
            "epoch 52/100, step 20/21, loss = 2.1433\n",
            "epoch 53/100, step 2/21, loss = 2.1901\n",
            "epoch 53/100, step 4/21, loss = 2.2156\n",
            "epoch 53/100, step 6/21, loss = 2.0933\n",
            "epoch 53/100, step 8/21, loss = 1.9652\n",
            "epoch 53/100, step 10/21, loss = 2.4370\n",
            "epoch 53/100, step 12/21, loss = 2.1144\n",
            "epoch 53/100, step 14/21, loss = 2.1471\n",
            "epoch 53/100, step 16/21, loss = 1.8358\n",
            "epoch 53/100, step 18/21, loss = 1.7846\n",
            "epoch 53/100, step 20/21, loss = 2.1056\n",
            "epoch 54/100, step 2/21, loss = 2.2385\n",
            "epoch 54/100, step 4/21, loss = 2.0407\n",
            "epoch 54/100, step 6/21, loss = 2.2148\n",
            "epoch 54/100, step 8/21, loss = 1.9048\n",
            "epoch 54/100, step 10/21, loss = 2.2191\n",
            "epoch 54/100, step 12/21, loss = 2.1015\n",
            "epoch 54/100, step 14/21, loss = 1.8770\n",
            "epoch 54/100, step 16/21, loss = 2.1794\n",
            "epoch 54/100, step 18/21, loss = 1.9687\n",
            "epoch 54/100, step 20/21, loss = 2.1359\n",
            "epoch 55/100, step 2/21, loss = 2.0524\n",
            "epoch 55/100, step 4/21, loss = 1.8674\n",
            "epoch 55/100, step 6/21, loss = 2.0807\n",
            "epoch 55/100, step 8/21, loss = 1.8958\n",
            "epoch 55/100, step 10/21, loss = 2.6536\n",
            "epoch 55/100, step 12/21, loss = 2.1199\n",
            "epoch 55/100, step 14/21, loss = 2.1600\n",
            "epoch 55/100, step 16/21, loss = 1.9465\n",
            "epoch 55/100, step 18/21, loss = 2.0301\n",
            "epoch 55/100, step 20/21, loss = 1.8393\n",
            "epoch 56/100, step 2/21, loss = 2.2041\n",
            "epoch 56/100, step 4/21, loss = 2.0542\n",
            "epoch 56/100, step 6/21, loss = 2.0114\n",
            "epoch 56/100, step 8/21, loss = 1.9579\n",
            "epoch 56/100, step 10/21, loss = 2.1532\n",
            "epoch 56/100, step 12/21, loss = 1.9978\n",
            "epoch 56/100, step 14/21, loss = 1.9470\n",
            "epoch 56/100, step 16/21, loss = 2.0353\n",
            "epoch 56/100, step 18/21, loss = 1.8653\n",
            "epoch 56/100, step 20/21, loss = 1.8763\n",
            "epoch 57/100, step 2/21, loss = 1.9981\n",
            "epoch 57/100, step 4/21, loss = 1.7708\n",
            "epoch 57/100, step 6/21, loss = 1.9078\n",
            "epoch 57/100, step 8/21, loss = 1.9670\n",
            "epoch 57/100, step 10/21, loss = 2.0173\n",
            "epoch 57/100, step 12/21, loss = 2.3476\n",
            "epoch 57/100, step 14/21, loss = 2.1653\n",
            "epoch 57/100, step 16/21, loss = 1.9416\n",
            "epoch 57/100, step 18/21, loss = 2.0059\n",
            "epoch 57/100, step 20/21, loss = 1.9977\n",
            "epoch 58/100, step 2/21, loss = 2.0601\n",
            "epoch 58/100, step 4/21, loss = 2.1907\n",
            "epoch 58/100, step 6/21, loss = 1.9764\n",
            "epoch 58/100, step 8/21, loss = 2.0864\n",
            "epoch 58/100, step 10/21, loss = 1.7906\n",
            "epoch 58/100, step 12/21, loss = 2.0712\n",
            "epoch 58/100, step 14/21, loss = 1.7704\n",
            "epoch 58/100, step 16/21, loss = 2.1314\n",
            "epoch 58/100, step 18/21, loss = 2.2096\n",
            "epoch 58/100, step 20/21, loss = 2.1072\n",
            "epoch 59/100, step 2/21, loss = 2.0003\n",
            "epoch 59/100, step 4/21, loss = 1.8376\n",
            "epoch 59/100, step 6/21, loss = 1.9422\n",
            "epoch 59/100, step 8/21, loss = 1.8992\n",
            "epoch 59/100, step 10/21, loss = 2.0855\n",
            "epoch 59/100, step 12/21, loss = 2.1521\n",
            "epoch 59/100, step 14/21, loss = 2.1493\n",
            "epoch 59/100, step 16/21, loss = 1.8481\n",
            "epoch 59/100, step 18/21, loss = 1.9122\n",
            "epoch 59/100, step 20/21, loss = 2.2544\n",
            "epoch 60/100, step 2/21, loss = 2.0273\n",
            "epoch 60/100, step 4/21, loss = 1.9835\n",
            "epoch 60/100, step 6/21, loss = 2.1712\n",
            "epoch 60/100, step 8/21, loss = 1.9525\n",
            "epoch 60/100, step 10/21, loss = 2.1824\n",
            "epoch 60/100, step 12/21, loss = 2.0542\n",
            "epoch 60/100, step 14/21, loss = 1.8966\n",
            "epoch 60/100, step 16/21, loss = 1.9776\n",
            "epoch 60/100, step 18/21, loss = 2.1298\n",
            "epoch 60/100, step 20/21, loss = 1.8319\n",
            "epoch 61/100, step 2/21, loss = 1.6891\n",
            "epoch 61/100, step 4/21, loss = 1.9975\n",
            "epoch 61/100, step 6/21, loss = 1.9392\n",
            "epoch 61/100, step 8/21, loss = 1.9503\n",
            "epoch 61/100, step 10/21, loss = 1.9917\n",
            "epoch 61/100, step 12/21, loss = 2.0846\n",
            "epoch 61/100, step 14/21, loss = 2.0839\n",
            "epoch 61/100, step 16/21, loss = 1.7779\n",
            "epoch 61/100, step 18/21, loss = 2.0684\n",
            "epoch 61/100, step 20/21, loss = 1.9387\n",
            "epoch 62/100, step 2/21, loss = 2.0066\n",
            "epoch 62/100, step 4/21, loss = 2.0513\n",
            "epoch 62/100, step 6/21, loss = 2.0455\n",
            "epoch 62/100, step 8/21, loss = 2.0875\n",
            "epoch 62/100, step 10/21, loss = 2.2204\n",
            "epoch 62/100, step 12/21, loss = 2.0800\n",
            "epoch 62/100, step 14/21, loss = 1.8981\n",
            "epoch 62/100, step 16/21, loss = 1.9825\n",
            "epoch 62/100, step 18/21, loss = 1.9324\n",
            "epoch 62/100, step 20/21, loss = 1.9671\n",
            "epoch 63/100, step 2/21, loss = 1.9199\n",
            "epoch 63/100, step 4/21, loss = 2.2353\n",
            "epoch 63/100, step 6/21, loss = 1.8350\n",
            "epoch 63/100, step 8/21, loss = 1.9340\n",
            "epoch 63/100, step 10/21, loss = 2.0509\n",
            "epoch 63/100, step 12/21, loss = 2.0363\n",
            "epoch 63/100, step 14/21, loss = 1.8209\n",
            "epoch 63/100, step 16/21, loss = 2.1302\n",
            "epoch 63/100, step 18/21, loss = 1.9447\n",
            "epoch 63/100, step 20/21, loss = 2.0147\n",
            "epoch 64/100, step 2/21, loss = 1.9746\n",
            "epoch 64/100, step 4/21, loss = 1.8896\n",
            "epoch 64/100, step 6/21, loss = 2.0395\n",
            "epoch 64/100, step 8/21, loss = 2.0092\n",
            "epoch 64/100, step 10/21, loss = 2.1316\n",
            "epoch 64/100, step 12/21, loss = 1.7469\n",
            "epoch 64/100, step 14/21, loss = 1.7776\n",
            "epoch 64/100, step 16/21, loss = 1.7516\n",
            "epoch 64/100, step 18/21, loss = 2.2184\n",
            "epoch 64/100, step 20/21, loss = 1.9559\n",
            "epoch 65/100, step 2/21, loss = 2.0301\n",
            "epoch 65/100, step 4/21, loss = 2.0781\n",
            "epoch 65/100, step 6/21, loss = 2.0347\n",
            "epoch 65/100, step 8/21, loss = 1.6660\n",
            "epoch 65/100, step 10/21, loss = 1.9528\n",
            "epoch 65/100, step 12/21, loss = 2.0385\n",
            "epoch 65/100, step 14/21, loss = 1.8214\n",
            "epoch 65/100, step 16/21, loss = 1.8350\n",
            "epoch 65/100, step 18/21, loss = 1.8463\n",
            "epoch 65/100, step 20/21, loss = 1.9535\n",
            "epoch 66/100, step 2/21, loss = 2.0814\n",
            "epoch 66/100, step 4/21, loss = 1.8846\n",
            "epoch 66/100, step 6/21, loss = 1.7637\n",
            "epoch 66/100, step 8/21, loss = 1.8517\n",
            "epoch 66/100, step 10/21, loss = 2.1697\n",
            "epoch 66/100, step 12/21, loss = 1.8259\n",
            "epoch 66/100, step 14/21, loss = 1.8099\n",
            "epoch 66/100, step 16/21, loss = 2.1670\n",
            "epoch 66/100, step 18/21, loss = 1.7862\n",
            "epoch 66/100, step 20/21, loss = 1.8439\n",
            "epoch 67/100, step 2/21, loss = 2.1495\n",
            "epoch 67/100, step 4/21, loss = 1.8071\n",
            "epoch 67/100, step 6/21, loss = 1.8198\n",
            "epoch 67/100, step 8/21, loss = 1.8627\n",
            "epoch 67/100, step 10/21, loss = 1.8525\n",
            "epoch 67/100, step 12/21, loss = 1.9064\n",
            "epoch 67/100, step 14/21, loss = 1.8261\n",
            "epoch 67/100, step 16/21, loss = 2.0425\n",
            "epoch 67/100, step 18/21, loss = 1.9920\n",
            "epoch 67/100, step 20/21, loss = 1.7284\n",
            "epoch 68/100, step 2/21, loss = 1.7069\n",
            "epoch 68/100, step 4/21, loss = 1.8023\n",
            "epoch 68/100, step 6/21, loss = 2.2540\n",
            "epoch 68/100, step 8/21, loss = 1.8861\n",
            "epoch 68/100, step 10/21, loss = 1.7730\n",
            "epoch 68/100, step 12/21, loss = 2.1848\n",
            "epoch 68/100, step 14/21, loss = 2.0064\n",
            "epoch 68/100, step 16/21, loss = 1.6428\n",
            "epoch 68/100, step 18/21, loss = 1.9372\n",
            "epoch 68/100, step 20/21, loss = 1.8080\n",
            "epoch 69/100, step 2/21, loss = 1.7173\n",
            "epoch 69/100, step 4/21, loss = 2.0455\n",
            "epoch 69/100, step 6/21, loss = 2.0134\n",
            "epoch 69/100, step 8/21, loss = 2.1001\n",
            "epoch 69/100, step 10/21, loss = 1.7552\n",
            "epoch 69/100, step 12/21, loss = 1.7266\n",
            "epoch 69/100, step 14/21, loss = 1.7612\n",
            "epoch 69/100, step 16/21, loss = 1.9223\n",
            "epoch 69/100, step 18/21, loss = 1.8375\n",
            "epoch 69/100, step 20/21, loss = 1.8881\n",
            "epoch 70/100, step 2/21, loss = 1.6930\n",
            "epoch 70/100, step 4/21, loss = 2.0745\n",
            "epoch 70/100, step 6/21, loss = 1.6391\n",
            "epoch 70/100, step 8/21, loss = 1.8614\n",
            "epoch 70/100, step 10/21, loss = 1.7829\n",
            "epoch 70/100, step 12/21, loss = 1.9168\n",
            "epoch 70/100, step 14/21, loss = 2.0536\n",
            "epoch 70/100, step 16/21, loss = 1.7519\n",
            "epoch 70/100, step 18/21, loss = 1.9281\n",
            "epoch 70/100, step 20/21, loss = 2.0955\n",
            "epoch 71/100, step 2/21, loss = 2.0184\n",
            "epoch 71/100, step 4/21, loss = 1.7644\n",
            "epoch 71/100, step 6/21, loss = 1.6558\n",
            "epoch 71/100, step 8/21, loss = 1.6150\n",
            "epoch 71/100, step 10/21, loss = 2.0613\n",
            "epoch 71/100, step 12/21, loss = 1.7600\n",
            "epoch 71/100, step 14/21, loss = 1.7129\n",
            "epoch 71/100, step 16/21, loss = 1.9511\n",
            "epoch 71/100, step 18/21, loss = 1.8958\n",
            "epoch 71/100, step 20/21, loss = 1.9893\n",
            "epoch 72/100, step 2/21, loss = 1.7536\n",
            "epoch 72/100, step 4/21, loss = 2.1837\n",
            "epoch 72/100, step 6/21, loss = 2.0495\n",
            "epoch 72/100, step 8/21, loss = 1.8503\n",
            "epoch 72/100, step 10/21, loss = 1.8000\n",
            "epoch 72/100, step 12/21, loss = 1.7382\n",
            "epoch 72/100, step 14/21, loss = 2.0386\n",
            "epoch 72/100, step 16/21, loss = 1.8445\n",
            "epoch 72/100, step 18/21, loss = 1.5437\n",
            "epoch 72/100, step 20/21, loss = 1.9040\n",
            "epoch 73/100, step 2/21, loss = 1.8080\n",
            "epoch 73/100, step 4/21, loss = 2.1178\n",
            "epoch 73/100, step 6/21, loss = 2.2026\n",
            "epoch 73/100, step 8/21, loss = 1.9741\n",
            "epoch 73/100, step 10/21, loss = 1.8278\n",
            "epoch 73/100, step 12/21, loss = 1.7934\n",
            "epoch 73/100, step 14/21, loss = 1.7006\n",
            "epoch 73/100, step 16/21, loss = 1.8283\n",
            "epoch 73/100, step 18/21, loss = 1.6792\n",
            "epoch 73/100, step 20/21, loss = 1.5789\n",
            "epoch 74/100, step 2/21, loss = 1.7302\n",
            "epoch 74/100, step 4/21, loss = 1.8178\n",
            "epoch 74/100, step 6/21, loss = 1.8231\n",
            "epoch 74/100, step 8/21, loss = 2.1756\n",
            "epoch 74/100, step 10/21, loss = 1.7587\n",
            "epoch 74/100, step 12/21, loss = 1.7421\n",
            "epoch 74/100, step 14/21, loss = 1.7524\n",
            "epoch 74/100, step 16/21, loss = 1.6238\n",
            "epoch 74/100, step 18/21, loss = 1.9807\n",
            "epoch 74/100, step 20/21, loss = 2.0105\n",
            "epoch 75/100, step 2/21, loss = 1.9636\n",
            "epoch 75/100, step 4/21, loss = 1.7557\n",
            "epoch 75/100, step 6/21, loss = 1.6770\n",
            "epoch 75/100, step 8/21, loss = 1.8662\n",
            "epoch 75/100, step 10/21, loss = 1.5871\n",
            "epoch 75/100, step 12/21, loss = 1.9025\n",
            "epoch 75/100, step 14/21, loss = 1.8557\n",
            "epoch 75/100, step 16/21, loss = 1.8529\n",
            "epoch 75/100, step 18/21, loss = 1.5437\n",
            "epoch 75/100, step 20/21, loss = 1.8512\n",
            "epoch 76/100, step 2/21, loss = 1.6551\n",
            "epoch 76/100, step 4/21, loss = 2.0393\n",
            "epoch 76/100, step 6/21, loss = 1.9033\n",
            "epoch 76/100, step 8/21, loss = 1.9760\n",
            "epoch 76/100, step 10/21, loss = 1.7194\n",
            "epoch 76/100, step 12/21, loss = 1.6169\n",
            "epoch 76/100, step 14/21, loss = 1.6398\n",
            "epoch 76/100, step 16/21, loss = 1.5117\n",
            "epoch 76/100, step 18/21, loss = 1.8660\n",
            "epoch 76/100, step 20/21, loss = 1.8903\n",
            "epoch 77/100, step 2/21, loss = 1.6718\n",
            "epoch 77/100, step 4/21, loss = 2.0767\n",
            "epoch 77/100, step 6/21, loss = 1.6457\n",
            "epoch 77/100, step 8/21, loss = 1.7826\n",
            "epoch 77/100, step 10/21, loss = 1.5110\n",
            "epoch 77/100, step 12/21, loss = 1.5014\n",
            "epoch 77/100, step 14/21, loss = 1.5217\n",
            "epoch 77/100, step 16/21, loss = 1.8655\n",
            "epoch 77/100, step 18/21, loss = 1.8656\n",
            "epoch 77/100, step 20/21, loss = 1.8385\n",
            "epoch 78/100, step 2/21, loss = 1.8856\n",
            "epoch 78/100, step 4/21, loss = 2.0505\n",
            "epoch 78/100, step 6/21, loss = 1.6486\n",
            "epoch 78/100, step 8/21, loss = 1.7823\n",
            "epoch 78/100, step 10/21, loss = 1.8716\n",
            "epoch 78/100, step 12/21, loss = 1.5937\n",
            "epoch 78/100, step 14/21, loss = 1.7427\n",
            "epoch 78/100, step 16/21, loss = 1.7275\n",
            "epoch 78/100, step 18/21, loss = 1.6680\n",
            "epoch 78/100, step 20/21, loss = 1.6704\n",
            "epoch 79/100, step 2/21, loss = 1.7255\n",
            "epoch 79/100, step 4/21, loss = 1.7322\n",
            "epoch 79/100, step 6/21, loss = 1.7684\n",
            "epoch 79/100, step 8/21, loss = 1.7894\n",
            "epoch 79/100, step 10/21, loss = 1.8339\n",
            "epoch 79/100, step 12/21, loss = 1.7827\n",
            "epoch 79/100, step 14/21, loss = 1.5378\n",
            "epoch 79/100, step 16/21, loss = 1.7728\n",
            "epoch 79/100, step 18/21, loss = 1.8843\n",
            "epoch 79/100, step 20/21, loss = 1.8296\n",
            "epoch 80/100, step 2/21, loss = 1.7892\n",
            "epoch 80/100, step 4/21, loss = 1.7898\n",
            "epoch 80/100, step 6/21, loss = 1.5408\n",
            "epoch 80/100, step 8/21, loss = 1.7159\n",
            "epoch 80/100, step 10/21, loss = 1.7193\n",
            "epoch 80/100, step 12/21, loss = 1.8339\n",
            "epoch 80/100, step 14/21, loss = 1.8015\n",
            "epoch 80/100, step 16/21, loss = 1.9708\n",
            "epoch 80/100, step 18/21, loss = 1.9014\n",
            "epoch 80/100, step 20/21, loss = 1.7910\n",
            "epoch 81/100, step 2/21, loss = 1.7476\n",
            "epoch 81/100, step 4/21, loss = 1.9303\n",
            "epoch 81/100, step 6/21, loss = 1.5489\n",
            "epoch 81/100, step 8/21, loss = 1.8716\n",
            "epoch 81/100, step 10/21, loss = 1.6530\n",
            "epoch 81/100, step 12/21, loss = 1.8585\n",
            "epoch 81/100, step 14/21, loss = 1.8422\n",
            "epoch 81/100, step 16/21, loss = 1.8024\n",
            "epoch 81/100, step 18/21, loss = 1.6133\n",
            "epoch 81/100, step 20/21, loss = 1.9655\n",
            "epoch 82/100, step 2/21, loss = 1.7385\n",
            "epoch 82/100, step 4/21, loss = 1.7211\n",
            "epoch 82/100, step 6/21, loss = 1.7182\n",
            "epoch 82/100, step 8/21, loss = 1.7678\n",
            "epoch 82/100, step 10/21, loss = 1.8471\n",
            "epoch 82/100, step 12/21, loss = 1.6529\n",
            "epoch 82/100, step 14/21, loss = 1.6451\n",
            "epoch 82/100, step 16/21, loss = 1.7056\n",
            "epoch 82/100, step 18/21, loss = 1.6529\n",
            "epoch 82/100, step 20/21, loss = 1.8034\n",
            "epoch 83/100, step 2/21, loss = 1.6009\n",
            "epoch 83/100, step 4/21, loss = 1.8851\n",
            "epoch 83/100, step 6/21, loss = 1.8064\n",
            "epoch 83/100, step 8/21, loss = 1.8375\n",
            "epoch 83/100, step 10/21, loss = 1.6737\n",
            "epoch 83/100, step 12/21, loss = 1.6709\n",
            "epoch 83/100, step 14/21, loss = 1.9169\n",
            "epoch 83/100, step 16/21, loss = 1.6906\n",
            "epoch 83/100, step 18/21, loss = 1.9150\n",
            "epoch 83/100, step 20/21, loss = 1.9376\n",
            "epoch 84/100, step 2/21, loss = 1.7674\n",
            "epoch 84/100, step 4/21, loss = 1.5099\n",
            "epoch 84/100, step 6/21, loss = 1.9117\n",
            "epoch 84/100, step 8/21, loss = 1.6935\n",
            "epoch 84/100, step 10/21, loss = 1.6543\n",
            "epoch 84/100, step 12/21, loss = 1.6757\n",
            "epoch 84/100, step 14/21, loss = 1.7687\n",
            "epoch 84/100, step 16/21, loss = 1.6026\n",
            "epoch 84/100, step 18/21, loss = 1.7812\n",
            "epoch 84/100, step 20/21, loss = 1.9029\n",
            "epoch 85/100, step 2/21, loss = 1.6569\n",
            "epoch 85/100, step 4/21, loss = 1.6747\n",
            "epoch 85/100, step 6/21, loss = 1.8731\n",
            "epoch 85/100, step 8/21, loss = 1.5840\n",
            "epoch 85/100, step 10/21, loss = 1.5354\n",
            "epoch 85/100, step 12/21, loss = 1.8601\n",
            "epoch 85/100, step 14/21, loss = 1.6574\n",
            "epoch 85/100, step 16/21, loss = 1.7052\n",
            "epoch 85/100, step 18/21, loss = 1.6763\n",
            "epoch 85/100, step 20/21, loss = 1.4673\n",
            "epoch 86/100, step 2/21, loss = 1.7498\n",
            "epoch 86/100, step 4/21, loss = 1.9153\n",
            "epoch 86/100, step 6/21, loss = 1.6952\n",
            "epoch 86/100, step 8/21, loss = 1.5937\n",
            "epoch 86/100, step 10/21, loss = 1.6686\n",
            "epoch 86/100, step 12/21, loss = 1.7480\n",
            "epoch 86/100, step 14/21, loss = 2.0310\n",
            "epoch 86/100, step 16/21, loss = 1.6447\n",
            "epoch 86/100, step 18/21, loss = 1.7030\n",
            "epoch 86/100, step 20/21, loss = 1.5934\n",
            "epoch 87/100, step 2/21, loss = 1.4762\n",
            "epoch 87/100, step 4/21, loss = 1.6544\n",
            "epoch 87/100, step 6/21, loss = 1.8550\n",
            "epoch 87/100, step 8/21, loss = 1.7277\n",
            "epoch 87/100, step 10/21, loss = 1.6125\n",
            "epoch 87/100, step 12/21, loss = 1.9231\n",
            "epoch 87/100, step 14/21, loss = 1.7160\n",
            "epoch 87/100, step 16/21, loss = 1.8251\n",
            "epoch 87/100, step 18/21, loss = 1.8442\n",
            "epoch 87/100, step 20/21, loss = 1.4977\n",
            "epoch 88/100, step 2/21, loss = 1.5615\n",
            "epoch 88/100, step 4/21, loss = 1.6852\n",
            "epoch 88/100, step 6/21, loss = 1.5638\n",
            "epoch 88/100, step 8/21, loss = 1.5927\n",
            "epoch 88/100, step 10/21, loss = 1.8792\n",
            "epoch 88/100, step 12/21, loss = 1.6922\n",
            "epoch 88/100, step 14/21, loss = 1.5838\n",
            "epoch 88/100, step 16/21, loss = 1.5871\n",
            "epoch 88/100, step 18/21, loss = 1.9808\n",
            "epoch 88/100, step 20/21, loss = 1.7551\n",
            "epoch 89/100, step 2/21, loss = 1.4230\n",
            "epoch 89/100, step 4/21, loss = 1.7872\n",
            "epoch 89/100, step 6/21, loss = 1.5812\n",
            "epoch 89/100, step 8/21, loss = 1.5552\n",
            "epoch 89/100, step 10/21, loss = 1.7694\n",
            "epoch 89/100, step 12/21, loss = 1.6943\n",
            "epoch 89/100, step 14/21, loss = 1.6987\n",
            "epoch 89/100, step 16/21, loss = 1.9654\n",
            "epoch 89/100, step 18/21, loss = 1.7283\n",
            "epoch 89/100, step 20/21, loss = 1.6954\n",
            "epoch 90/100, step 2/21, loss = 1.5886\n",
            "epoch 90/100, step 4/21, loss = 1.7780\n",
            "epoch 90/100, step 6/21, loss = 1.8166\n",
            "epoch 90/100, step 8/21, loss = 1.7315\n",
            "epoch 90/100, step 10/21, loss = 1.6006\n",
            "epoch 90/100, step 12/21, loss = 1.5694\n",
            "epoch 90/100, step 14/21, loss = 1.7325\n",
            "epoch 90/100, step 16/21, loss = 1.4221\n",
            "epoch 90/100, step 18/21, loss = 1.7934\n",
            "epoch 90/100, step 20/21, loss = 1.6117\n",
            "epoch 91/100, step 2/21, loss = 1.8686\n",
            "epoch 91/100, step 4/21, loss = 1.7245\n",
            "epoch 91/100, step 6/21, loss = 1.4688\n",
            "epoch 91/100, step 8/21, loss = 1.7672\n",
            "epoch 91/100, step 10/21, loss = 1.4501\n",
            "epoch 91/100, step 12/21, loss = 1.8981\n",
            "epoch 91/100, step 14/21, loss = 1.8354\n",
            "epoch 91/100, step 16/21, loss = 1.5774\n",
            "epoch 91/100, step 18/21, loss = 1.7638\n",
            "epoch 91/100, step 20/21, loss = 1.6621\n",
            "epoch 92/100, step 2/21, loss = 1.8507\n",
            "epoch 92/100, step 4/21, loss = 1.3506\n",
            "epoch 92/100, step 6/21, loss = 1.5605\n",
            "epoch 92/100, step 8/21, loss = 1.8182\n",
            "epoch 92/100, step 10/21, loss = 1.6681\n",
            "epoch 92/100, step 12/21, loss = 1.8290\n",
            "epoch 92/100, step 14/21, loss = 1.7401\n",
            "epoch 92/100, step 16/21, loss = 1.7624\n",
            "epoch 92/100, step 18/21, loss = 1.3923\n",
            "epoch 92/100, step 20/21, loss = 1.8678\n",
            "epoch 93/100, step 2/21, loss = 1.6278\n",
            "epoch 93/100, step 4/21, loss = 1.4975\n",
            "epoch 93/100, step 6/21, loss = 1.6766\n",
            "epoch 93/100, step 8/21, loss = 1.6716\n",
            "epoch 93/100, step 10/21, loss = 1.7517\n",
            "epoch 93/100, step 12/21, loss = 1.5994\n",
            "epoch 93/100, step 14/21, loss = 1.8556\n",
            "epoch 93/100, step 16/21, loss = 1.8537\n",
            "epoch 93/100, step 18/21, loss = 1.4742\n",
            "epoch 93/100, step 20/21, loss = 1.8297\n",
            "epoch 94/100, step 2/21, loss = 1.8112\n",
            "epoch 94/100, step 4/21, loss = 1.6319\n",
            "epoch 94/100, step 6/21, loss = 1.7071\n",
            "epoch 94/100, step 8/21, loss = 1.7423\n",
            "epoch 94/100, step 10/21, loss = 1.5528\n",
            "epoch 94/100, step 12/21, loss = 1.4974\n",
            "epoch 94/100, step 14/21, loss = 1.6422\n",
            "epoch 94/100, step 16/21, loss = 1.9124\n",
            "epoch 94/100, step 18/21, loss = 1.7630\n",
            "epoch 94/100, step 20/21, loss = 1.6634\n",
            "epoch 95/100, step 2/21, loss = 1.3956\n",
            "epoch 95/100, step 4/21, loss = 1.9840\n",
            "epoch 95/100, step 6/21, loss = 1.7704\n",
            "epoch 95/100, step 8/21, loss = 1.8560\n",
            "epoch 95/100, step 10/21, loss = 1.8920\n",
            "epoch 95/100, step 12/21, loss = 1.6531\n",
            "epoch 95/100, step 14/21, loss = 1.6099\n",
            "epoch 95/100, step 16/21, loss = 1.5621\n",
            "epoch 95/100, step 18/21, loss = 1.7182\n",
            "epoch 95/100, step 20/21, loss = 1.4395\n",
            "epoch 96/100, step 2/21, loss = 1.7994\n",
            "epoch 96/100, step 4/21, loss = 1.6454\n",
            "epoch 96/100, step 6/21, loss = 1.7168\n",
            "epoch 96/100, step 8/21, loss = 1.5123\n",
            "epoch 96/100, step 10/21, loss = 1.6096\n",
            "epoch 96/100, step 12/21, loss = 1.5611\n",
            "epoch 96/100, step 14/21, loss = 1.6530\n",
            "epoch 96/100, step 16/21, loss = 1.7033\n",
            "epoch 96/100, step 18/21, loss = 1.6001\n",
            "epoch 96/100, step 20/21, loss = 1.4512\n",
            "epoch 97/100, step 2/21, loss = 1.6279\n",
            "epoch 97/100, step 4/21, loss = 1.4069\n",
            "epoch 97/100, step 6/21, loss = 1.6051\n",
            "epoch 97/100, step 8/21, loss = 1.6864\n",
            "epoch 97/100, step 10/21, loss = 1.4310\n",
            "epoch 97/100, step 12/21, loss = 1.4894\n",
            "epoch 97/100, step 14/21, loss = 1.6179\n",
            "epoch 97/100, step 16/21, loss = 1.9186\n",
            "epoch 97/100, step 18/21, loss = 1.6335\n",
            "epoch 97/100, step 20/21, loss = 1.7321\n",
            "epoch 98/100, step 2/21, loss = 1.4507\n",
            "epoch 98/100, step 4/21, loss = 1.4425\n",
            "epoch 98/100, step 6/21, loss = 1.5418\n",
            "epoch 98/100, step 8/21, loss = 2.0577\n",
            "epoch 98/100, step 10/21, loss = 1.8025\n",
            "epoch 98/100, step 12/21, loss = 1.5699\n",
            "epoch 98/100, step 14/21, loss = 1.9301\n",
            "epoch 98/100, step 16/21, loss = 1.6654\n",
            "epoch 98/100, step 18/21, loss = 1.5284\n",
            "epoch 98/100, step 20/21, loss = 1.7942\n",
            "epoch 99/100, step 2/21, loss = 1.6713\n",
            "epoch 99/100, step 4/21, loss = 1.7494\n",
            "epoch 99/100, step 6/21, loss = 1.4485\n",
            "epoch 99/100, step 8/21, loss = 1.5623\n",
            "epoch 99/100, step 10/21, loss = 1.6978\n",
            "epoch 99/100, step 12/21, loss = 1.7531\n",
            "epoch 99/100, step 14/21, loss = 1.7900\n",
            "epoch 99/100, step 16/21, loss = 1.8530\n",
            "epoch 99/100, step 18/21, loss = 1.6822\n",
            "epoch 99/100, step 20/21, loss = 1.5302\n",
            "epoch 100/100, step 2/21, loss = 1.5334\n",
            "epoch 100/100, step 4/21, loss = 1.5453\n",
            "epoch 100/100, step 6/21, loss = 1.8306\n",
            "epoch 100/100, step 8/21, loss = 1.7214\n",
            "epoch 100/100, step 10/21, loss = 1.5215\n",
            "epoch 100/100, step 12/21, loss = 1.7342\n",
            "epoch 100/100, step 14/21, loss = 1.3680\n",
            "epoch 100/100, step 16/21, loss = 1.8748\n",
            "epoch 100/100, step 18/21, loss = 1.6168\n",
            "epoch 100/100, step 20/21, loss = 1.7641\n",
            "Finished training.\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "print(\"Total Steps:\",n_total_steps)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (name,numeric,cat1,cat2,cat3,cat4,target) in enumerate(train_loader):\n",
        "        \n",
        "        # --- CRITICAL STEP: Move all input tensors to the GPU ---\n",
        "        numeric = numeric.to(device)\n",
        "        cat1 = cat1.to(device)\n",
        "        cat2 = cat2.to(device)\n",
        "        cat3 = cat3.to(device)\n",
        "        cat4 = cat4.to(device)\n",
        "        target = target.to(device)\n",
        "        # --------------------------------------------------------\n",
        "        \n",
        "        outputs = model(numeric,cat1,cat2,cat3,cat4)\n",
        "        loss = criterion(outputs,target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 2 == 0:\n",
        "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 20.00% (23/115)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for name,numeric,cat1,cat2,cat3,cat4,target in test_loader:\n",
        "        # Move test data to GPU as well\n",
        "        numeric = numeric.to(device)\n",
        "        cat1 = cat1.to(device)\n",
        "        cat2 = cat2.to(device)\n",
        "        cat3 = cat3.to(device)\n",
        "        cat4 = cat4.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        outputs = model(numeric,cat1,cat2,cat3,cat4)\n",
        "\n",
        "        _, predictions = torch.max(outputs,1) # 1 is the dimension\n",
        "        n_samples += target.shape[0]\n",
        "        n_correct += (predictions == target).sum().item()\n",
        "    \n",
        "    accuracy = 100 * n_correct / n_samples\n",
        "    print(f'accuracy = {accuracy:.2f}% ({n_correct}/{n_samples})')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pydml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
