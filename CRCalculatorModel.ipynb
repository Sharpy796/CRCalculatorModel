{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kbKiDiKvAi9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pMLIk29tK2bc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import scipy\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning) # TODO: Actually optimize the source of this warning\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUBv09tBIEL"
      },
      "source": [
        "# Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "zj0dSPtbBHm6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "size 6\n",
            "alignment 17\n",
            "type 15\n",
            "legendary 2\n",
            "input size 85\n"
          ]
        }
      ],
      "source": [
        "# Device configuration, this is to check if GPU is available and run on GPU\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 85\n",
        "hidden_size = 100 # number of nodes in hidden layer\n",
        "num_classes = 34 # number of classes, 0, 1/8, 1/4, 1/2, 1-30\n",
        "num_epochs = 4 # number of times we go through the entire dataset\n",
        "batch_size = 64 # number of samples in one forward/backward pass\n",
        "learning_rate = 0.001 # learning rate\n",
        "train_val = .9\n",
        "\n",
        "\n",
        "class MonsterDataset(Dataset):\n",
        "    def __init__(self, csv_file, train, train_val=.9, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.CAT_COLS = ['size','alignment','type','legendary']\n",
        "        self.NONNUMERIC_COLS = ['size','alignment','type','legendary','name','attributes','actions','legendary_actions']\n",
        "        # Add CR mapping\n",
        "        self.CR_TO_IDX = {\n",
        "            0: 0, 0.125: 1, 0.25: 2, 0.5: 3,\n",
        "            1: 4, 2: 5, 3: 6, 4: 7, 5: 8, 6: 9, 7: 10, 8: 11, 9: 12, 10: 13,\n",
        "            11: 14, 12: 15, 13: 16, 14: 17, 15: 18, 16: 19, 17: 20, 18: 21,\n",
        "            19: 22, 20: 23, 21: 24, 22: 25, 23: 26, 24: 27, 25: 28, 26: 29,\n",
        "            27: 30, 28: 31, 29: 32, 30: 33\n",
        "        }\n",
        "        self.__parsecsv__(csv_file)\n",
        "        if train:\n",
        "            self.df = self.df.iloc[1:int(self.df.shape[0]*train_val)+1].reset_index(drop=True)\n",
        "        else:\n",
        "            self.df = self.df.iloc[int(self.df.shape[0]*train_val):self.df.shape[0]].reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __parsecsv__(self, csv_file):\n",
        "        self.df_original = pd.read_csv(csv_file)\n",
        "        self.df = self.df_original.copy()\n",
        "        self.original_categorical_vals = pd.DataFrame()\n",
        "\n",
        "        self.__reclassify_categorical__('size')\n",
        "        self.__reclassify_categorical__('alignment')\n",
        "        self.__reclassify_categorical__('type')\n",
        "        self.__reclassify_categorical__('legendary')\n",
        "        self.__reclassify_list__('languages', \", \")\n",
        "        self.__reclassify_list__('senses', \", \")\n",
        "\n",
        "        # temporary removing of string values so I can work only on num values\n",
        "        self.df = self.df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "        # remove source because these don't contribute anything\n",
        "        self.df = self.df.drop(['source'],axis=1)\n",
        "        \n",
        "        self.__redefine_datatypes__()\n",
        "\n",
        "        for col in self.CAT_COLS:\n",
        "            self.dummify_cat_values(col)\n",
        "        \n",
        "    def dummify_cat_values(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        dummies = pd.get_dummies(df_copy[col],prefix=col).astype('float32')\n",
        "        df_copy = pd.concat([df_copy,dummies],axis=1)\n",
        "        df_copy = df_copy.drop([col],axis=1)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __update_ocv__(self, df, col, unique):\n",
        "        self.original_categorical_vals = pd.concat([self.original_categorical_vals, pd.DataFrame({col:unique})], axis=1)\n",
        "\n",
        "    def __redefine_datatypes__(self):\n",
        "        df_copy = self.df.copy()\n",
        "        for each in df_copy.columns:\n",
        "            if each in self.CAT_COLS:\n",
        "                df_copy[each] = df_copy[each].astype('category')\n",
        "            elif each == 'name':\n",
        "                pass\n",
        "            else:\n",
        "                df_copy[each] = pd.to_numeric(df_copy[each], errors='coerce').astype(np.float32)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_categorical__(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        if col == 'type':\n",
        "            for i,each in enumerate(df_copy[col]):\n",
        "                if \"(\" in each:\n",
        "                    df_copy.at[i,col] = each[:(each.find(\"(\")-1)]\n",
        "        elif col == 'alignment': # TODO: reduce dimensionality for alignment\n",
        "            for i,each in enumerate(df_copy[col].unique()):\n",
        "                # if each not in \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\":\n",
        "                #     val = \"\"\n",
        "                #     if \"any\" in each:\n",
        "                #         if \"non\" in each:\n",
        "                #             if \"-good\" in each:\n",
        "                #                 val = \"lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                #             elif \"-lawful\" in each:\n",
        "                #                 val = \"neutral good,chaotic good,neutral,chaotic neutral,neutral evil,chaotic evil\"\n",
        "                #         elif \"evil\" in each:\n",
        "                #             val = \"lawful evil,neutral evil,chaotic evil\"\n",
        "                #         elif \"chaotic\" in each:\n",
        "                #             val = \"chaotic good,chaotic neutral,chaotic evil\"\n",
        "                #         else:\n",
        "                #             val = \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                #     elif \"or\" in each:\n",
        "                #         if \"neutral good\" in each and \"neutral evil\" in each:\n",
        "                #             val = \"neutral good,neutral evil\"\n",
        "                #         elif \"chaotic good\" in each and \"neutral evil\" in each:\n",
        "                #             val = \"chaotic good,neutral evil\"\n",
        "                #     df_copy.at[i,col] = val\n",
        "                pass\n",
        "\n",
        "\n",
        "        unique = df_copy[col].unique()\n",
        "        # if col == 'alignment': print(unique)\n",
        "        self.__update_ocv__(df_copy, col, unique)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_list__(self, col, delimiter):\n",
        "        df_copy = self.df.copy()\n",
        "        column = df_copy[col]\n",
        "        for i in range(0,len(column)):\n",
        "            num = 0\n",
        "            item = column[i]\n",
        "            vals = item.split(delimiter)\n",
        "            for each in vals:\n",
        "                each = each.lower()\n",
        "                if \"two\" in each: num = num + 2\n",
        "                elif \"three\" in each: num = num + 3\n",
        "                elif \"four\" in each: num = num + 4\n",
        "                elif \"five\" in each: num = num + 5\n",
        "                else: num = num + 1\n",
        "            df_copy.at[i,col] = num\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def getocv(self):\n",
        "        return self.original_categorical_vals\n",
        "    \n",
        "    def create_subdf(self,substring):\n",
        "        df_copy = self.df.copy()\n",
        "        subdf = pd.DataFrame()\n",
        "        for each in df_copy:\n",
        "            if substring in each:\n",
        "                subdf = pd.concat([subdf,df_copy[each]],axis=1)\n",
        "        return subdf\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        df_copy = self.df.copy()\n",
        "        \n",
        "        monster_name = df_copy['name']\n",
        "        cat_size = self.create_subdf(\"size\")\n",
        "        cat_type = self.create_subdf(\"type\")\n",
        "        cat_alignment = self.create_subdf(\"alignment\")\n",
        "        cat_legendary = self.create_subdf(\"legendary\")\n",
        "        numeric = df_copy.copy()\n",
        "        numeric = numeric.drop(cat_size.columns,axis=1)\n",
        "        numeric = numeric.drop(cat_type.columns,axis=1)\n",
        "        numeric = numeric.drop(cat_alignment.columns,axis=1)\n",
        "        numeric = numeric.drop(cat_legendary.columns,axis=1)\n",
        "        numeric = numeric.drop(['name','cr'],axis=1)\n",
        "        target_value = df_copy['cr']\n",
        "\n",
        "        monster_name = monster_name[idx]\n",
        "        cat_size = cat_size.iloc[idx]\n",
        "        cat_type = cat_type.iloc[idx]\n",
        "        cat_alignment = cat_alignment.iloc[idx]\n",
        "        cat_legendary = cat_legendary.iloc[idx]\n",
        "        numeric = numeric.iloc[idx]\n",
        "        target_value = target_value[idx]\n",
        "\n",
        "        cat_size = torch.tensor(cat_size, dtype=torch.long)\n",
        "        cat_type = torch.tensor(cat_type, dtype=torch.long)\n",
        "        cat_alignment = torch.tensor(cat_alignment, dtype=torch.long)\n",
        "        cat_legendary = torch.tensor(cat_legendary, dtype=torch.long)\n",
        "        numeric = torch.tensor(numeric, dtype=torch.float32)\n",
        "        target_value = float(target_value)\n",
        "        target_value = torch.tensor(self.CR_TO_IDX[target_value], dtype=torch.long)\n",
        "\n",
        "        return monster_name,numeric,cat_size,cat_type,cat_alignment,cat_legendary,target_value\n",
        "    \n",
        "    def getdf(self):\n",
        "        return self.df\n",
        "\n",
        "train_dataset = MonsterDataset(\"aidedd_blocks2.csv\",train=True,train_val=train_val)\n",
        "test_dataset = MonsterDataset(\"aidedd_blocks2.csv\",train=False,train_val=train_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "train_df = train_dataset.getdf()\n",
        "\n",
        "input_size = len(train_df.columns) # 85 including all the expanded categorical data\n",
        "\n",
        "ocv = train_dataset.getocv()\n",
        "for each in ocv:\n",
        "    print(each, len(ocv[each].dropna()))\n",
        "print(\"input size\",input_size)\n",
        "# ocv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6oUMaolB8ge",
        "outputId": "45483e63-8ca6-4d51-ebc3-55043de025c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, None) torch.Size([100, 83])\n",
            "(None,) torch.Size([100])\n",
            "(None, None) torch.Size([34, 100])\n",
            "(None,) torch.Size([34])\n"
          ]
        }
      ],
      "source": [
        "class CRPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(CRPredictor,self).__init__()\n",
        "        self.l1 = nn.Linear(83,hidden_size) # first layer\n",
        "        self.relu = nn.ReLU() # activation function\n",
        "        self.l2 = nn.Linear(hidden_size,num_classes) # second layer\n",
        "        # self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, numeric, cat1,cat2,cat3,cat4):\n",
        "        x = torch.cat([numeric, cat1,cat2,cat3,cat4],dim=1)\n",
        "        x = self.l1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.l2(x)\n",
        "        # x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = CRPredictor(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for param in model.parameters():\n",
        "    print(param.names, param.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "WWfsBaoGhNoz",
        "outputId": "16cf988b-4935-41ed-b44c-90df8cfe0eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Steps: 11\n",
            "epoch 1/4, step 2/11, loss = 7.7607\n",
            "epoch 1/4, step 4/11, loss = 6.2508\n",
            "epoch 1/4, step 6/11, loss = 5.5990\n",
            "epoch 1/4, step 8/11, loss = 4.2099\n",
            "epoch 1/4, step 10/11, loss = 4.8254\n",
            "epoch 2/4, step 2/11, loss = 4.6241\n",
            "epoch 2/4, step 4/11, loss = 3.4686\n",
            "epoch 2/4, step 6/11, loss = 3.6797\n",
            "epoch 2/4, step 8/11, loss = 3.5026\n",
            "epoch 2/4, step 10/11, loss = 3.4802\n",
            "epoch 3/4, step 2/11, loss = 2.9655\n",
            "epoch 3/4, step 4/11, loss = 2.7804\n",
            "epoch 3/4, step 6/11, loss = 3.0137\n",
            "epoch 3/4, step 8/11, loss = 2.8675\n",
            "epoch 3/4, step 10/11, loss = 2.9627\n",
            "epoch 4/4, step 2/11, loss = 2.8922\n",
            "epoch 4/4, step 4/11, loss = 2.5101\n",
            "epoch 4/4, step 6/11, loss = 2.4638\n",
            "epoch 4/4, step 8/11, loss = 2.6000\n",
            "epoch 4/4, step 10/11, loss = 2.4820\n",
            "Finished training.\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "print(\"Total Steps:\",n_total_steps)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (name,numeric,cat1,cat2,cat3,cat4,target) in enumerate(train_loader):\n",
        "        \n",
        "        outputs = model(numeric,cat1,cat2,cat3,cat4)\n",
        "        loss = criterion(outputs,target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 2 == 0:\n",
        "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 15.584415584415584 (12/77)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for name,numeric,cat1,cat2,cat3,cat4,target in test_loader:\n",
        "        outputs = model(numeric,cat1,cat2,cat3,cat4)\n",
        "\n",
        "        _, predictions = torch.max(outputs,1) # 1 is the dimension\n",
        "        n_samples += target.shape[0]\n",
        "        n_correct += (predictions == target).sum().item()\n",
        "    \n",
        "    accuracy = 100 * n_correct / n_samples\n",
        "    print(f'accuracy = {accuracy} ({n_correct}/{n_samples})')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aienv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
