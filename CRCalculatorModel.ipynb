{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kbKiDiKvAi9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "pMLIk29tK2bc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch_directml\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import scipy\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning) # TODO: Actually optimize the source of this warning\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUBv09tBIEL"
      },
      "source": [
        "# Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: privateuseone:0\n"
          ]
        }
      ],
      "source": [
        "# Device configuration, this is to check if GPU is available and run on GPU\n",
        "# device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "# print(f\"Using {device} device\")\n",
        "# We use torch_directml.device() to grab the first DirectX12 capable GPU.\n",
        "device = torch_directml.device()\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zj0dSPtbBHm6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "size 6\n",
            "alignment 17\n",
            "type 15\n",
            "legendary 2\n",
            "input size 85\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameters\n",
        "input_size = 85\n",
        "hidden_size = 100 # number of nodes in hidden layer\n",
        "num_classes = 34 # number of classes, 0, 1/8, 1/4, 1/2, 1-30\n",
        "num_epochs = 64 # number of times we go through the entire dataset\n",
        "batch_size = 64 # number of samples in one forward/backward pass\n",
        "learning_rate = 0.001 # learning rate\n",
        "train_val = .80\n",
        "\n",
        "\n",
        "class MonsterDataset(Dataset):\n",
        "    def __init__(self, csv_file, train, train_val=.9, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.CAT_COLS = ['size','alignment','type','legendary']\n",
        "        self.NONNUMERIC_COLS = ['size','alignment','type','legendary','name','attributes','actions','legendary_actions']\n",
        "        # Add CR mapping\n",
        "        self.CR_TO_IDX = {\n",
        "            0: 0, 0.125: 1, 0.25: 2, 0.5: 3,\n",
        "            1: 4, 2: 5, 3: 6, 4: 7, 5: 8, 6: 9, 7: 10, 8: 11, 9: 12, 10: 13,\n",
        "            11: 14, 12: 15, 13: 16, 14: 17, 15: 18, 16: 19, 17: 20, 18: 21,\n",
        "            19: 22, 20: 23, 21: 24, 22: 25, 23: 26, 24: 27, 25: 28, 26: 29,\n",
        "            27: 30, 28: 31, 29: 32, 30: 33\n",
        "        }\n",
        "        self.__parsecsv__(csv_file)\n",
        "        if train:\n",
        "            self.df = self.df.iloc[1:int(self.df.shape[0]*train_val)+1].reset_index(drop=True)\n",
        "        else:\n",
        "            self.df = self.df.iloc[int(self.df.shape[0]*train_val):self.df.shape[0]].reset_index(drop=True)\n",
        "\n",
        "        # Create sub dfs of categorized data\n",
        "        df_size = self.create_subdf(\"size\")\n",
        "        df_type = self.create_subdf(\"type\")\n",
        "        df_align = self.create_subdf(\"alignment\")\n",
        "        df_legend = self.create_subdf(\"legendary\")\n",
        "\n",
        "        cols_to_drop = list(df_size.columns) + list(df_type.columns) + \\\n",
        "                       list(df_align.columns) + list(df_legend.columns) + \\\n",
        "                       ['name', 'cr']\n",
        "        # Only drop columns that actually exist to avoid errors\n",
        "        cols_to_drop = [c for c in cols_to_drop if c in self.df.columns]\n",
        "        df_numeric = self.df.drop(cols_to_drop, axis=1)\n",
        "\n",
        "        # Convert everything to Tensors NOW\n",
        "        # .values converts pandas to numpy, which creates tensors much faster\n",
        "        self.names = self.df['name'].values\n",
        "\n",
        "        self.data_size = torch.tensor(df_size.values, dtype=torch.long)\n",
        "        self.data_type = torch.tensor(df_type.values, dtype=torch.long)\n",
        "        self.data_align = torch.tensor(df_align.values, dtype=torch.long)\n",
        "        self.data_legend = torch.tensor(df_legend.values, dtype=torch.long)\n",
        "\n",
        "        # Pre-calculate targets\n",
        "        self.data_numeric = torch.tensor(df_numeric.values, dtype=torch.float32)# Map the CR values to indices using a list comprehension or map\n",
        "        targets = self.df['cr'].map(self.CR_TO_IDX).fillna(0).values \n",
        "        self.data_targets = torch.tensor(targets, dtype=torch.long)\n",
        "\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __parsecsv__(self, csv_file):\n",
        "        self.df_original = pd.read_csv(csv_file)\n",
        "        self.df = self.df_original.copy()\n",
        "        self.original_categorical_vals = pd.DataFrame()\n",
        "\n",
        "        self.__reclassify_categorical__('size')\n",
        "        self.__reclassify_categorical__('alignment')\n",
        "        self.__reclassify_categorical__('type')\n",
        "        self.__reclassify_categorical__('legendary')\n",
        "        self.__reclassify_list__('languages', \", \")\n",
        "        self.__reclassify_list__('senses', \", \")\n",
        "\n",
        "        # temporary removing of string values so I can work only on num values\n",
        "        self.df = self.df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "        # remove source because these don't contribute anything\n",
        "        self.df = self.df.drop(['source'],axis=1)\n",
        "        \n",
        "        self.__redefine_datatypes__()\n",
        "\n",
        "        for col in self.CAT_COLS:\n",
        "            self.dummify_cat_values(col)\n",
        "        \n",
        "    def dummify_cat_values(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        dummies = pd.get_dummies(df_copy[col],prefix=col).astype('float32')\n",
        "        df_copy = pd.concat([df_copy,dummies],axis=1)\n",
        "        df_copy = df_copy.drop([col],axis=1)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __update_ocv__(self, df, col, unique):\n",
        "        self.original_categorical_vals = pd.concat([self.original_categorical_vals, pd.DataFrame({col:unique})], axis=1)\n",
        "\n",
        "    def __redefine_datatypes__(self):\n",
        "        df_copy = self.df.copy()\n",
        "        for each in df_copy.columns:\n",
        "            if each in self.CAT_COLS:\n",
        "                df_copy[each] = df_copy[each].astype('category')\n",
        "            elif each == 'name':\n",
        "                pass\n",
        "            else:\n",
        "                df_copy[each] = pd.to_numeric(df_copy[each], errors='coerce').astype(np.float32)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_categorical__(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        if col == 'type':\n",
        "            for i,each in enumerate(df_copy[col]):\n",
        "                if \"(\" in each:\n",
        "                    df_copy.at[i,col] = each[:(each.find(\"(\")-1)]\n",
        "        elif col == 'alignment': # TODO: reduce dimensionality for alignment\n",
        "            for i,each in enumerate(df_copy[col].unique()):\n",
        "                # if each not in \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\":\n",
        "                #     val = \"\"\n",
        "                #     if \"any\" in each:\n",
        "                #         if \"non\" in each:\n",
        "                #             if \"-good\" in each:\n",
        "                #                 val = \"lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                #             elif \"-lawful\" in each:\n",
        "                #                 val = \"neutral good,chaotic good,neutral,chaotic neutral,neutral evil,chaotic evil\"\n",
        "                #         elif \"evil\" in each:\n",
        "                #             val = \"lawful evil,neutral evil,chaotic evil\"\n",
        "                #         elif \"chaotic\" in each:\n",
        "                #             val = \"chaotic good,chaotic neutral,chaotic evil\"\n",
        "                #         else:\n",
        "                #             val = \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                #     elif \"or\" in each:\n",
        "                #         if \"neutral good\" in each and \"neutral evil\" in each:\n",
        "                #             val = \"neutral good,neutral evil\"\n",
        "                #         elif \"chaotic good\" in each and \"neutral evil\" in each:\n",
        "                #             val = \"chaotic good,neutral evil\"\n",
        "                #     df_copy.at[i,col] = val\n",
        "                pass\n",
        "\n",
        "\n",
        "        unique = df_copy[col].unique()\n",
        "        # if col == 'alignment': print(unique)\n",
        "        self.__update_ocv__(df_copy, col, unique)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_list__(self, col, delimiter):\n",
        "        df_copy = self.df.copy()\n",
        "        column = df_copy[col]\n",
        "        for i in range(0,len(column)):\n",
        "            num = 0\n",
        "            item = column[i]\n",
        "            vals = item.split(delimiter)\n",
        "            for each in vals:\n",
        "                each = each.lower()\n",
        "                if \"two\" in each: num = num + 2\n",
        "                elif \"three\" in each: num = num + 3\n",
        "                elif \"four\" in each: num = num + 4\n",
        "                elif \"five\" in each: num = num + 5\n",
        "                else: num = num + 1\n",
        "            df_copy.at[i,col] = num\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    def getocv(self):\n",
        "        return self.original_categorical_vals\n",
        "    \n",
        "\n",
        "    def create_subdf(self,substring):\n",
        "        '''\n",
        "        Isolates a whole section of the dataframe by creating a copy\n",
        "        of it and concatenating columns containing the desired substring.\n",
        "        Meant for creating sub-dataframes of categorized one-hot encoded data.\n",
        "        '''\n",
        "        df_copy = self.df.copy()\n",
        "        subdf = pd.DataFrame()\n",
        "        for each in df_copy:\n",
        "            if substring in each:\n",
        "                subdf = pd.concat([subdf,df_copy[each]],axis=1)\n",
        "        return subdf\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        return (\n",
        "            self.names[idx],\n",
        "            self.data_numeric[idx],\n",
        "            self.data_size[idx],\n",
        "            self.data_type[idx],\n",
        "            self.data_align[idx],\n",
        "            self.data_legend[idx],\n",
        "            self.data_targets[idx]\n",
        "        )\n",
        "    \n",
        "    def getdf(self):\n",
        "        return self.df\n",
        "\n",
        "train_dataset = MonsterDataset(\"aidedd_blocks2.csv\",train=True,train_val=train_val)\n",
        "test_dataset = MonsterDataset(\"aidedd_blocks2.csv\",train=False,train_val=train_val)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "train_df = train_dataset.getdf()\n",
        "\n",
        "input_size = len(train_df.columns) # 85 including all the expanded categorical data\n",
        "\n",
        "ocv = train_dataset.getocv()\n",
        "for each in ocv:\n",
        "    print(each, len(ocv[each].dropna()))\n",
        "print(\"input size\",input_size)\n",
        "# ocv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6oUMaolB8ge",
        "outputId": "45483e63-8ca6-4d51-ebc3-55043de025c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, None) torch.Size([100, 83])\n",
            "(None,) torch.Size([100])\n",
            "(None, None) torch.Size([100, 100])\n",
            "(None,) torch.Size([100])\n",
            "(None, None) torch.Size([100, 100])\n",
            "(None,) torch.Size([100])\n",
            "(None, None) torch.Size([34, 100])\n",
            "(None,) torch.Size([34])\n"
          ]
        }
      ],
      "source": [
        "class CRPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(CRPredictor,self).__init__()\n",
        "        self.l1 = nn.Linear(83,hidden_size) # first layer\n",
        "        self.relu1 = nn.ReLU() # activation function\n",
        "        self.l2 = nn.Linear(hidden_size,hidden_size) # second layer\n",
        "        self.relu2 = nn.ReLU() # activation function\n",
        "        self.l3 = nn.Linear(hidden_size,hidden_size) # third layer\n",
        "        self.relu3 = nn.ReLU() # activation function\n",
        "        self.l4 = nn.Linear(hidden_size,num_classes) # fourth layer\n",
        "        self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, numeric, cat1,cat2,cat3,cat4):\n",
        "        x = torch.cat([numeric, cat1,cat2,cat3,cat4],dim=1)\n",
        "        x = self.l1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.l3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.l4(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = CRPredictor(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for param in model.parameters():\n",
        "    print(param.names, param.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "WWfsBaoGhNoz",
        "outputId": "16cf988b-4935-41ed-b44c-90df8cfe0eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total Steps: 10\n",
            "epoch 1/64, step 2/10, loss = 3.5265\n",
            "epoch 1/64, step 4/10, loss = 3.5224\n",
            "epoch 1/64, step 6/10, loss = 3.5028\n",
            "epoch 1/64, step 8/10, loss = 3.5018\n",
            "epoch 1/64, step 10/10, loss = 3.5153\n",
            "epoch 2/64, step 2/10, loss = 3.4936\n",
            "epoch 2/64, step 4/10, loss = 3.4933\n",
            "epoch 2/64, step 6/10, loss = 3.5183\n",
            "epoch 2/64, step 8/10, loss = 3.4361\n",
            "epoch 2/64, step 10/10, loss = 3.4808\n",
            "epoch 3/64, step 2/10, loss = 3.3877\n",
            "epoch 3/64, step 4/10, loss = 3.4023\n",
            "epoch 3/64, step 6/10, loss = 3.4369\n",
            "epoch 3/64, step 8/10, loss = 3.4451\n",
            "epoch 3/64, step 10/10, loss = 3.4250\n",
            "epoch 4/64, step 2/10, loss = 3.3974\n",
            "epoch 4/64, step 4/10, loss = 3.3276\n",
            "epoch 4/64, step 6/10, loss = 3.3904\n",
            "epoch 4/64, step 8/10, loss = 3.4529\n",
            "epoch 4/64, step 10/10, loss = 3.3918\n",
            "epoch 5/64, step 2/10, loss = 3.4301\n",
            "epoch 5/64, step 4/10, loss = 3.3238\n",
            "epoch 5/64, step 6/10, loss = 3.4866\n",
            "epoch 5/64, step 8/10, loss = 3.3795\n",
            "epoch 5/64, step 10/10, loss = 3.4304\n",
            "epoch 6/64, step 2/10, loss = 3.3985\n",
            "epoch 6/64, step 4/10, loss = 3.3816\n",
            "epoch 6/64, step 6/10, loss = 3.4184\n",
            "epoch 6/64, step 8/10, loss = 3.3255\n",
            "epoch 6/64, step 10/10, loss = 3.3714\n",
            "epoch 7/64, step 2/10, loss = 3.3693\n",
            "epoch 7/64, step 4/10, loss = 3.3979\n",
            "epoch 7/64, step 6/10, loss = 3.3064\n",
            "epoch 7/64, step 8/10, loss = 3.3111\n",
            "epoch 7/64, step 10/10, loss = 3.2312\n",
            "epoch 8/64, step 2/10, loss = 3.3842\n",
            "epoch 8/64, step 4/10, loss = 3.3078\n",
            "epoch 8/64, step 6/10, loss = 3.2237\n",
            "epoch 8/64, step 8/10, loss = 3.3665\n",
            "epoch 8/64, step 10/10, loss = 3.3861\n",
            "epoch 9/64, step 2/10, loss = 3.3840\n",
            "epoch 9/64, step 4/10, loss = 3.3011\n",
            "epoch 9/64, step 6/10, loss = 3.3276\n",
            "epoch 9/64, step 8/10, loss = 3.2385\n",
            "epoch 9/64, step 10/10, loss = 3.3117\n",
            "epoch 10/64, step 2/10, loss = 3.3197\n",
            "epoch 10/64, step 4/10, loss = 3.2989\n",
            "epoch 10/64, step 6/10, loss = 3.2929\n",
            "epoch 10/64, step 8/10, loss = 3.2898\n",
            "epoch 10/64, step 10/10, loss = 3.3300\n",
            "epoch 11/64, step 2/10, loss = 3.2859\n",
            "epoch 11/64, step 4/10, loss = 3.2964\n",
            "epoch 11/64, step 6/10, loss = 3.3150\n",
            "epoch 11/64, step 8/10, loss = 3.2887\n",
            "epoch 11/64, step 10/10, loss = 3.3180\n",
            "epoch 12/64, step 2/10, loss = 3.2771\n",
            "epoch 12/64, step 4/10, loss = 3.2379\n",
            "epoch 12/64, step 6/10, loss = 3.2603\n",
            "epoch 12/64, step 8/10, loss = 3.3128\n",
            "epoch 12/64, step 10/10, loss = 3.3186\n",
            "epoch 13/64, step 2/10, loss = 3.3688\n",
            "epoch 13/64, step 4/10, loss = 3.2777\n",
            "epoch 13/64, step 6/10, loss = 3.3200\n",
            "epoch 13/64, step 8/10, loss = 3.1744\n",
            "epoch 13/64, step 10/10, loss = 3.1757\n",
            "epoch 14/64, step 2/10, loss = 3.1636\n",
            "epoch 14/64, step 4/10, loss = 3.3416\n",
            "epoch 14/64, step 6/10, loss = 3.2223\n",
            "epoch 14/64, step 8/10, loss = 3.2150\n",
            "epoch 14/64, step 10/10, loss = 3.3075\n",
            "epoch 15/64, step 2/10, loss = 3.1785\n",
            "epoch 15/64, step 4/10, loss = 3.2219\n",
            "epoch 15/64, step 6/10, loss = 3.3289\n",
            "epoch 15/64, step 8/10, loss = 3.2738\n",
            "epoch 15/64, step 10/10, loss = 3.2963\n",
            "epoch 16/64, step 2/10, loss = 3.2225\n",
            "epoch 16/64, step 4/10, loss = 3.2805\n",
            "epoch 16/64, step 6/10, loss = 3.2563\n",
            "epoch 16/64, step 8/10, loss = 3.2451\n",
            "epoch 16/64, step 10/10, loss = 3.2222\n",
            "epoch 17/64, step 2/10, loss = 3.3203\n",
            "epoch 17/64, step 4/10, loss = 3.2481\n",
            "epoch 17/64, step 6/10, loss = 3.3505\n",
            "epoch 17/64, step 8/10, loss = 3.1954\n",
            "epoch 17/64, step 10/10, loss = 3.2524\n",
            "epoch 18/64, step 2/10, loss = 3.2647\n",
            "epoch 18/64, step 4/10, loss = 3.2283\n",
            "epoch 18/64, step 6/10, loss = 3.2458\n",
            "epoch 18/64, step 8/10, loss = 3.2745\n",
            "epoch 18/64, step 10/10, loss = 3.3208\n",
            "epoch 19/64, step 2/10, loss = 3.2000\n",
            "epoch 19/64, step 4/10, loss = 3.2946\n",
            "epoch 19/64, step 6/10, loss = 3.3019\n",
            "epoch 19/64, step 8/10, loss = 3.2632\n",
            "epoch 19/64, step 10/10, loss = 3.2396\n",
            "epoch 20/64, step 2/10, loss = 3.2893\n",
            "epoch 20/64, step 4/10, loss = 3.2321\n",
            "epoch 20/64, step 6/10, loss = 3.2471\n",
            "epoch 20/64, step 8/10, loss = 3.3117\n",
            "epoch 20/64, step 10/10, loss = 3.1921\n",
            "epoch 21/64, step 2/10, loss = 3.2169\n",
            "epoch 21/64, step 4/10, loss = 3.3404\n",
            "epoch 21/64, step 6/10, loss = 3.2582\n",
            "epoch 21/64, step 8/10, loss = 3.1606\n",
            "epoch 21/64, step 10/10, loss = 3.2296\n",
            "epoch 22/64, step 2/10, loss = 3.2046\n",
            "epoch 22/64, step 4/10, loss = 3.2144\n",
            "epoch 22/64, step 6/10, loss = 3.3090\n",
            "epoch 22/64, step 8/10, loss = 3.1871\n",
            "epoch 22/64, step 10/10, loss = 3.2141\n",
            "epoch 23/64, step 2/10, loss = 3.3109\n",
            "epoch 23/64, step 4/10, loss = 3.2524\n",
            "epoch 23/64, step 6/10, loss = 3.2053\n",
            "epoch 23/64, step 8/10, loss = 3.1879\n",
            "epoch 23/64, step 10/10, loss = 3.1666\n",
            "epoch 24/64, step 2/10, loss = 3.2428\n",
            "epoch 24/64, step 4/10, loss = 3.2191\n",
            "epoch 24/64, step 6/10, loss = 3.2169\n",
            "epoch 24/64, step 8/10, loss = 3.2858\n",
            "epoch 24/64, step 10/10, loss = 3.1301\n",
            "epoch 25/64, step 2/10, loss = 3.1590\n",
            "epoch 25/64, step 4/10, loss = 3.3441\n",
            "epoch 25/64, step 6/10, loss = 3.2999\n",
            "epoch 25/64, step 8/10, loss = 3.2045\n",
            "epoch 25/64, step 10/10, loss = 3.1745\n",
            "epoch 26/64, step 2/10, loss = 3.2367\n",
            "epoch 26/64, step 4/10, loss = 3.1635\n",
            "epoch 26/64, step 6/10, loss = 3.2874\n",
            "epoch 26/64, step 8/10, loss = 3.2109\n",
            "epoch 26/64, step 10/10, loss = 3.2930\n",
            "epoch 27/64, step 2/10, loss = 3.2828\n",
            "epoch 27/64, step 4/10, loss = 3.1659\n",
            "epoch 27/64, step 6/10, loss = 3.1551\n",
            "epoch 27/64, step 8/10, loss = 3.2601\n",
            "epoch 27/64, step 10/10, loss = 3.1068\n",
            "epoch 28/64, step 2/10, loss = 3.2124\n",
            "epoch 28/64, step 4/10, loss = 3.1647\n",
            "epoch 28/64, step 6/10, loss = 3.2070\n",
            "epoch 28/64, step 8/10, loss = 3.2268\n",
            "epoch 28/64, step 10/10, loss = 3.2469\n",
            "epoch 29/64, step 2/10, loss = 3.1178\n",
            "epoch 29/64, step 4/10, loss = 3.1922\n",
            "epoch 29/64, step 6/10, loss = 3.1671\n",
            "epoch 29/64, step 8/10, loss = 3.2469\n",
            "epoch 29/64, step 10/10, loss = 3.2123\n",
            "epoch 30/64, step 2/10, loss = 3.1782\n",
            "epoch 30/64, step 4/10, loss = 3.1745\n",
            "epoch 30/64, step 6/10, loss = 3.2073\n",
            "epoch 30/64, step 8/10, loss = 3.1765\n",
            "epoch 30/64, step 10/10, loss = 3.2428\n",
            "epoch 31/64, step 2/10, loss = 3.2679\n",
            "epoch 31/64, step 4/10, loss = 3.2473\n",
            "epoch 31/64, step 6/10, loss = 3.2976\n",
            "epoch 31/64, step 8/10, loss = 3.1920\n",
            "epoch 31/64, step 10/10, loss = 3.2211\n",
            "epoch 32/64, step 2/10, loss = 3.2207\n",
            "epoch 32/64, step 4/10, loss = 3.1577\n",
            "epoch 32/64, step 6/10, loss = 3.1820\n",
            "epoch 32/64, step 8/10, loss = 3.2559\n",
            "epoch 32/64, step 10/10, loss = 3.2074\n",
            "epoch 33/64, step 2/10, loss = 3.1538\n",
            "epoch 33/64, step 4/10, loss = 3.1568\n",
            "epoch 33/64, step 6/10, loss = 3.2452\n",
            "epoch 33/64, step 8/10, loss = 3.2259\n",
            "epoch 33/64, step 10/10, loss = 3.1995\n",
            "epoch 34/64, step 2/10, loss = 3.2323\n",
            "epoch 34/64, step 4/10, loss = 3.2085\n",
            "epoch 34/64, step 6/10, loss = 3.2335\n",
            "epoch 34/64, step 8/10, loss = 3.3136\n",
            "epoch 34/64, step 10/10, loss = 3.1257\n",
            "epoch 35/64, step 2/10, loss = 3.1069\n",
            "epoch 35/64, step 4/10, loss = 3.2328\n",
            "epoch 35/64, step 6/10, loss = 3.0874\n",
            "epoch 35/64, step 8/10, loss = 3.3518\n",
            "epoch 35/64, step 10/10, loss = 3.1128\n",
            "epoch 36/64, step 2/10, loss = 3.1163\n",
            "epoch 36/64, step 4/10, loss = 3.2126\n",
            "epoch 36/64, step 6/10, loss = 3.1948\n",
            "epoch 36/64, step 8/10, loss = 3.2421\n",
            "epoch 36/64, step 10/10, loss = 3.2111\n",
            "epoch 37/64, step 2/10, loss = 3.1459\n",
            "epoch 37/64, step 4/10, loss = 3.2097\n",
            "epoch 37/64, step 6/10, loss = 3.2367\n",
            "epoch 37/64, step 8/10, loss = 3.1164\n",
            "epoch 37/64, step 10/10, loss = 3.2571\n",
            "epoch 38/64, step 2/10, loss = 3.2157\n",
            "epoch 38/64, step 4/10, loss = 3.1325\n",
            "epoch 38/64, step 6/10, loss = 3.1388\n",
            "epoch 38/64, step 8/10, loss = 3.1752\n",
            "epoch 38/64, step 10/10, loss = 3.2745\n",
            "epoch 39/64, step 2/10, loss = 3.2637\n",
            "epoch 39/64, step 4/10, loss = 3.2917\n",
            "epoch 39/64, step 6/10, loss = 3.2302\n",
            "epoch 39/64, step 8/10, loss = 3.1442\n",
            "epoch 39/64, step 10/10, loss = 3.1228\n",
            "epoch 40/64, step 2/10, loss = 3.2309\n",
            "epoch 40/64, step 4/10, loss = 3.1856\n",
            "epoch 40/64, step 6/10, loss = 3.1485\n",
            "epoch 40/64, step 8/10, loss = 3.2002\n",
            "epoch 40/64, step 10/10, loss = 3.1745\n",
            "epoch 41/64, step 2/10, loss = 3.2920\n",
            "epoch 41/64, step 4/10, loss = 3.2194\n",
            "epoch 41/64, step 6/10, loss = 3.2334\n",
            "epoch 41/64, step 8/10, loss = 3.1481\n",
            "epoch 41/64, step 10/10, loss = 3.2997\n",
            "epoch 42/64, step 2/10, loss = 3.1323\n",
            "epoch 42/64, step 4/10, loss = 3.2126\n",
            "epoch 42/64, step 6/10, loss = 3.2330\n",
            "epoch 42/64, step 8/10, loss = 3.1963\n",
            "epoch 42/64, step 10/10, loss = 3.3033\n",
            "epoch 43/64, step 2/10, loss = 3.1464\n",
            "epoch 43/64, step 4/10, loss = 3.2505\n",
            "epoch 43/64, step 6/10, loss = 3.0831\n",
            "epoch 43/64, step 8/10, loss = 3.1964\n",
            "epoch 43/64, step 10/10, loss = 3.1213\n",
            "epoch 44/64, step 2/10, loss = 3.1930\n",
            "epoch 44/64, step 4/10, loss = 3.1599\n",
            "epoch 44/64, step 6/10, loss = 3.2578\n",
            "epoch 44/64, step 8/10, loss = 3.1238\n",
            "epoch 44/64, step 10/10, loss = 3.2375\n",
            "epoch 45/64, step 2/10, loss = 3.2071\n",
            "epoch 45/64, step 4/10, loss = 3.2179\n",
            "epoch 45/64, step 6/10, loss = 3.1585\n",
            "epoch 45/64, step 8/10, loss = 3.1268\n",
            "epoch 45/64, step 10/10, loss = 3.1155\n",
            "epoch 46/64, step 2/10, loss = 3.1825\n",
            "epoch 46/64, step 4/10, loss = 3.1105\n",
            "epoch 46/64, step 6/10, loss = 3.2157\n",
            "epoch 46/64, step 8/10, loss = 3.1622\n",
            "epoch 46/64, step 10/10, loss = 3.2485\n",
            "epoch 47/64, step 2/10, loss = 3.2653\n",
            "epoch 47/64, step 4/10, loss = 3.2397\n",
            "epoch 47/64, step 6/10, loss = 3.1834\n",
            "epoch 47/64, step 8/10, loss = 3.1951\n",
            "epoch 47/64, step 10/10, loss = 3.1551\n",
            "epoch 48/64, step 2/10, loss = 3.0367\n",
            "epoch 48/64, step 4/10, loss = 3.1992\n",
            "epoch 48/64, step 6/10, loss = 3.1926\n",
            "epoch 48/64, step 8/10, loss = 3.1939\n",
            "epoch 48/64, step 10/10, loss = 3.1786\n",
            "epoch 49/64, step 2/10, loss = 3.1848\n",
            "epoch 49/64, step 4/10, loss = 3.2031\n",
            "epoch 49/64, step 6/10, loss = 3.1548\n",
            "epoch 49/64, step 8/10, loss = 3.0379\n",
            "epoch 49/64, step 10/10, loss = 3.1453\n",
            "epoch 50/64, step 2/10, loss = 3.1756\n",
            "epoch 50/64, step 4/10, loss = 3.1320\n",
            "epoch 50/64, step 6/10, loss = 3.1776\n",
            "epoch 50/64, step 8/10, loss = 3.1365\n",
            "epoch 50/64, step 10/10, loss = 3.1319\n",
            "epoch 51/64, step 2/10, loss = 3.0766\n",
            "epoch 51/64, step 4/10, loss = 3.1247\n",
            "epoch 51/64, step 6/10, loss = 3.2337\n",
            "epoch 51/64, step 8/10, loss = 3.2043\n",
            "epoch 51/64, step 10/10, loss = 3.3918\n",
            "epoch 52/64, step 2/10, loss = 3.2271\n",
            "epoch 52/64, step 4/10, loss = 3.1823\n",
            "epoch 52/64, step 6/10, loss = 3.1376\n",
            "epoch 52/64, step 8/10, loss = 3.1011\n",
            "epoch 52/64, step 10/10, loss = 3.2049\n",
            "epoch 53/64, step 2/10, loss = 3.2234\n",
            "epoch 53/64, step 4/10, loss = 3.1367\n",
            "epoch 53/64, step 6/10, loss = 3.1329\n",
            "epoch 53/64, step 8/10, loss = 3.2519\n",
            "epoch 53/64, step 10/10, loss = 3.1408\n",
            "epoch 54/64, step 2/10, loss = 3.1943\n",
            "epoch 54/64, step 4/10, loss = 3.2319\n",
            "epoch 54/64, step 6/10, loss = 3.1028\n",
            "epoch 54/64, step 8/10, loss = 3.1925\n",
            "epoch 54/64, step 10/10, loss = 3.0457\n",
            "epoch 55/64, step 2/10, loss = 3.2418\n",
            "epoch 55/64, step 4/10, loss = 3.2748\n",
            "epoch 55/64, step 6/10, loss = 3.1763\n",
            "epoch 55/64, step 8/10, loss = 3.0934\n",
            "epoch 55/64, step 10/10, loss = 3.1980\n",
            "epoch 56/64, step 2/10, loss = 3.1427\n",
            "epoch 56/64, step 4/10, loss = 3.1586\n",
            "epoch 56/64, step 6/10, loss = 3.2239\n",
            "epoch 56/64, step 8/10, loss = 3.2830\n",
            "epoch 56/64, step 10/10, loss = 3.1084\n",
            "epoch 57/64, step 2/10, loss = 3.2136\n",
            "epoch 57/64, step 4/10, loss = 3.1358\n",
            "epoch 57/64, step 6/10, loss = 3.1926\n",
            "epoch 57/64, step 8/10, loss = 3.1085\n",
            "epoch 57/64, step 10/10, loss = 3.2763\n",
            "epoch 58/64, step 2/10, loss = 3.1550\n",
            "epoch 58/64, step 4/10, loss = 3.0934\n",
            "epoch 58/64, step 6/10, loss = 3.2208\n",
            "epoch 58/64, step 8/10, loss = 3.2632\n",
            "epoch 58/64, step 10/10, loss = 3.2156\n",
            "epoch 59/64, step 2/10, loss = 3.2035\n",
            "epoch 59/64, step 4/10, loss = 3.2445\n",
            "epoch 59/64, step 6/10, loss = 3.2203\n",
            "epoch 59/64, step 8/10, loss = 3.1053\n",
            "epoch 59/64, step 10/10, loss = 2.9861\n",
            "epoch 60/64, step 2/10, loss = 3.0912\n",
            "epoch 60/64, step 4/10, loss = 3.1242\n",
            "epoch 60/64, step 6/10, loss = 3.2224\n",
            "epoch 60/64, step 8/10, loss = 3.2665\n",
            "epoch 60/64, step 10/10, loss = 3.1201\n",
            "epoch 61/64, step 2/10, loss = 3.1770\n",
            "epoch 61/64, step 4/10, loss = 3.1887\n",
            "epoch 61/64, step 6/10, loss = 3.2058\n",
            "epoch 61/64, step 8/10, loss = 3.2088\n",
            "epoch 61/64, step 10/10, loss = 3.2205\n",
            "epoch 62/64, step 2/10, loss = 3.0250\n",
            "epoch 62/64, step 4/10, loss = 3.1781\n",
            "epoch 62/64, step 6/10, loss = 3.1955\n",
            "epoch 62/64, step 8/10, loss = 3.2598\n",
            "epoch 62/64, step 10/10, loss = 3.3711\n",
            "epoch 63/64, step 2/10, loss = 3.2381\n",
            "epoch 63/64, step 4/10, loss = 3.1142\n",
            "epoch 63/64, step 6/10, loss = 3.1463\n",
            "epoch 63/64, step 8/10, loss = 3.1492\n",
            "epoch 63/64, step 10/10, loss = 3.1828\n",
            "epoch 64/64, step 2/10, loss = 3.1816\n",
            "epoch 64/64, step 4/10, loss = 3.1039\n",
            "epoch 64/64, step 6/10, loss = 3.1767\n",
            "epoch 64/64, step 8/10, loss = 3.0649\n",
            "epoch 64/64, step 10/10, loss = 3.1624\n",
            "Finished training.\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "print(\"Total Steps:\",n_total_steps)\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (name,numeric,cat1,cat2,cat3,cat4,target) in enumerate(train_loader):\n",
        "        \n",
        "        # --- CRITICAL STEP: Move all input tensors to the GPU ---\n",
        "        numeric = numeric.to(device)\n",
        "        cat1 = cat1.to(device)\n",
        "        cat2 = cat2.to(device)\n",
        "        cat3 = cat3.to(device)\n",
        "        cat4 = cat4.to(device)\n",
        "        target = target.to(device)\n",
        "        # --------------------------------------------------------\n",
        "        \n",
        "        outputs = model(numeric,cat1,cat2,cat3,cat4)\n",
        "        loss = criterion(outputs,target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 2 == 0:\n",
        "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "accuracy = 24.18300653594771 (37/153)\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # we don't need gradients in the testing phase\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    for name,numeric,cat1,cat2,cat3,cat4,target in test_loader:\n",
        "        # Move test data to GPU as well\n",
        "        numeric = numeric.to(device)\n",
        "        cat1 = cat1.to(device)\n",
        "        cat2 = cat2.to(device)\n",
        "        cat3 = cat3.to(device)\n",
        "        cat4 = cat4.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        outputs = model(numeric,cat1,cat2,cat3,cat4)\n",
        "\n",
        "        _, predictions = torch.max(outputs,1) # 1 is the dimension\n",
        "        n_samples += target.shape[0]\n",
        "        n_correct += (predictions == target).sum().item()\n",
        "    \n",
        "    accuracy = 100 * n_correct / n_samples\n",
        "    print(f'accuracy = {accuracy} ({n_correct}/{n_samples})')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pywml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
