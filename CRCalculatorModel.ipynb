{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kbKiDiKvAi9"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 391,
      "metadata": {
        "id": "pMLIk29tK2bc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import scipy\n",
        "from scipy.stats import zscore\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "from warnings import simplefilter\n",
        "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning) # TODO: Actually optimize the source of this warning\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(\"ignore\", UserWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xJKA-xPvDPf"
      },
      "source": [
        "# Import Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 392,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpJwQSI5Li6s",
        "outputId": "bb76207a-3a37-4568-d694-9eaec72fcca3"
      },
      "outputs": [],
      "source": [
        "# # Set the path to the file you'd like to load\n",
        "# file_path = \"aidedd_blocks2.csv\"\n",
        "\n",
        "# # Load the latest version\n",
        "# df_original = kagglehub.dataset_load(\n",
        "#   KaggleDatasetAdapter.PANDAS,\n",
        "#   \"travistyler/dnd-5e-monster-manual-stats\",\n",
        "#   file_path,\n",
        "#   # Provide any additional arguments like\n",
        "#   # sql_query or pandas_kwargs. See the\n",
        "#   # documenation for more information:\n",
        "#   # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        "# )\n",
        "\n",
        "# df = df_original.copy()\n",
        "# # Create dataframe to keep track of the original values of each categorical data\n",
        "# original_categorical_vals = pd.DataFrame()\n",
        "\n",
        "# def update_ocv(col, unique):\n",
        "#   global original_categorical_vals\n",
        "#   original_categorical_vals = pd.concat([original_categorical_vals, pd.DataFrame({col:unique})], axis=1)\n",
        "\n",
        "# def __reclassify_categorical__(df, col):\n",
        "#   df_copy = df.copy()\n",
        "#   unique = df_copy[col].unique()\n",
        "#   update_ocv(col, unique)\n",
        "#   for i in range(0,len(unique)):\n",
        "#     df_copy = df_copy.replace({col: unique[i]}, i)\n",
        "#   return df_copy\n",
        "\n",
        "# def view_categorical_legend(col):\n",
        "#   return original_categorical_vals[col].dropna()\n",
        "\n",
        "# def __reclassify_list__(df, col, delimiter):\n",
        "#   df_copy = df.copy()\n",
        "#   column = df_copy[col]\n",
        "#   for i in range(0,len(column)):\n",
        "#     num = 0\n",
        "#     item = column[i]\n",
        "#     vals = item.split(delimiter)\n",
        "#     for each in vals:\n",
        "#       each = each.lower()\n",
        "#       if \"two\" in each: num = num + 2\n",
        "#       elif \"three\" in each: num = num + 3\n",
        "#       elif \"four\" in each: num = num + 4\n",
        "#       elif \"five\" in each: num = num + 5\n",
        "#       else: num = num + 1\n",
        "#     df_copy.at[i,col] = num\n",
        "#   return df_copy\n",
        "\n",
        "# # # print(df.dtypes)\n",
        "# # TODO: Play with category types??\n",
        "# # df_test = df['size'].astype('category')\n",
        "\n",
        "\n",
        "# # df = reclassify_categorical(df, 'size')\n",
        "# # df = reclassify_categorical(df, 'alignment')\n",
        "# # df = reclassify_categorical(df, 'type')\n",
        "# df = __reclassify_list__(df, 'languages', \", \")\n",
        "# df = __reclassify_list__(df, 'senses', \", \")\n",
        "# # print(df['attributes'][2].split(\" | \"))\n",
        "# # print(df['actions'][2].split(\" | \"))\n",
        "# # print(df['legendary_actions'][2].split(\" | \"))\n",
        "# # view_categorical_legend('alignment')\n",
        "\n",
        "# # temporary removing of string values so I can work only on num values\n",
        "# df = df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "# # remove name and source because these don't contribute anything\n",
        "# df = df.drop(['name','source'],axis=1)\n",
        "# # fix numeric values into floats\n",
        "# # for each in df.columns:\n",
        "# #   if each not in original_categorical_vals.columns:\n",
        "# #     df[each] = df[each].astype(np.float32)\n",
        "# #   else:\n",
        "# #     # df[each] = df[each].astype(int) # apparently it doesn't like integers\n",
        "# #     df[each] = df[each].astype(np.float32)\n",
        "\n",
        "# # # categorical_cols = original_categorical_vals.columns\n",
        "# # target_col = 'cr'\n",
        "# # categorical_cols = ['size','alignment','type']\n",
        "# # numerical_cols = df.drop(categorical_cols,axis=1).columns\n",
        "# # numerical_cols = df.drop([target_col],axis=1).columns\n",
        "# # cat_maps = {}\n",
        "\n",
        "# # for col in categorical_cols:\n",
        "# #     unique_vals = df[col].unique()\n",
        "# #     cat_maps[col] = {val: i for i, val in enumerate(unique_vals)}\n",
        "\n",
        "# # for col in categorical_cols:\n",
        "# #     df[col] = df[col].map(cat_maps[col]).astype(int)\n",
        "\n",
        "# # items = ['numerical','categorical','cr']\n",
        "# # data_list = [] # this is the data that we are going to be using\n",
        "\n",
        "# # for _, row in df.iterrows():\n",
        "# #     data_list.append({\n",
        "# #         \"numerical\": row[numerical_cols].astype(float).tolist(),\n",
        "# #         \"categorical\": {col: int(row[col]) for col in categorical_cols},\n",
        "# #         \"cr\": float(row[target_col])\n",
        "# #     })\n",
        "\n",
        "# # def show_monster(monster):\n",
        "# #   print(f'Numerical:\\t{data_list[monster]['numerical']}')\n",
        "# #   print(f'Categorical:\\t{data_list[monster]['categorical']}')\n",
        "# #   print(f'CR:\\t\\t{data_list[monster]['cr']}')\n",
        "\n",
        "# # show_monster(1)\n",
        "# # show_monster(2)\n",
        "# # show_monster(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPUBv09tBIEL"
      },
      "source": [
        "# Pytorch Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 393,
      "metadata": {
        "id": "zj0dSPtbBHm6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cpu device\n",
            "size 6\n",
            "alignment 17\n",
            "type 15\n",
            "legendary 2\n",
            "input size 85\n"
          ]
        }
      ],
      "source": [
        "# Device configuration, this is to check if GPU is available and run on GPU\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Hyperparameters\n",
        "# input_size = len(df.columns) # 48\n",
        "input_size = 85\n",
        "hidden_size = 100 # number of nodes in hidden layer\n",
        "num_classes = 33 # number of classes, 0, 1/4, 1/2, 1-30\n",
        "num_epochs = 2 # number of times we go through the entire dataset\n",
        "batch_size = 100 # number of samples in one forward/backward pass\n",
        "learning_rate = 0.001 # learning rate\n",
        "\n",
        "\n",
        "class MonsterDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            root_dir (string): Directory with all the images.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.CAT_COLS = ['size','alignment','type','legendary']\n",
        "        self.NONNUMERIC_COLS = ['size','alignment','type','legendary','name','attributes','actions','legendary_actions']\n",
        "        self.__parsecsv__(csv_file)\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __parsecsv__(self, csv_file):\n",
        "        self.df_original = pd.read_csv(csv_file)\n",
        "        self.df = self.df_original.copy()\n",
        "        self.original_categorical_vals = pd.DataFrame()\n",
        "\n",
        "        self.__reclassify_categorical__('size')\n",
        "        self.__reclassify_categorical__('alignment')\n",
        "        self.__reclassify_categorical__('type')\n",
        "        self.__reclassify_categorical__('legendary')\n",
        "        self.__reclassify_list__('languages', \", \")\n",
        "        self.__reclassify_list__('senses', \", \")\n",
        "\n",
        "        # temporary removing of string values so I can work only on num values\n",
        "        self.df = self.df.drop(['attributes','actions','legendary_actions'],axis=1)\n",
        "        # remove source because these don't contribute anything\n",
        "        self.df = self.df.drop(['source'],axis=1)\n",
        "        \n",
        "        self.__redefine_datatypes__()\n",
        "\n",
        "        for col in self.CAT_COLS:\n",
        "            self.dummify_cat_values(col)\n",
        "\n",
        "        # z-score normalization (if we need it)\n",
        "        # num_cols = self.df.select_dtypes('float32').columns\n",
        "        # for each in num_cols:\n",
        "        #     if each != 'cr':\n",
        "        #         self.normalize_zscore(each)\n",
        "        \n",
        "    def dummify_cat_values(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        dummies = pd.get_dummies(df_copy[col],prefix=col).astype('float32')\n",
        "        df_copy = pd.concat([df_copy,dummies],axis=1)\n",
        "        df_copy = df_copy.drop([col],axis=1)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __update_ocv__(self, df, col, unique):\n",
        "        self.original_categorical_vals = pd.concat([self.original_categorical_vals, pd.DataFrame({col:unique})], axis=1)\n",
        "\n",
        "    def __redefine_datatypes__(self):\n",
        "        df_copy = self.df.copy()\n",
        "        for each in df_copy.columns:\n",
        "            if each in self.CAT_COLS:\n",
        "                df_copy[each] = df_copy[each].astype('category')\n",
        "            elif each == 'name':\n",
        "                pass\n",
        "            else:\n",
        "                df_copy[each] = pd.to_numeric(df_copy[each], errors='coerce').astype(np.float32)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_categorical__(self, col):\n",
        "        df_copy = self.df.copy()\n",
        "        if col == 'type':\n",
        "            for i,each in enumerate(df_copy[col]):\n",
        "                if \"(\" in each:\n",
        "                    df_copy.at[i,col] = each[:(each.find(\"(\")-1)]\n",
        "        elif col == 'alignment': # TODO: reduce dimensionality for alignment\n",
        "            for i,each in enumerate(df_copy[col].unique()):\n",
        "                # if each not in \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\":\n",
        "                #     val = \"\"\n",
        "                #     if \"any\" in each:\n",
        "                #         if \"non\" in each:\n",
        "                #             if \"-good\" in each:\n",
        "                #                 val = \"lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                #             elif \"-lawful\" in each:\n",
        "                #                 val = \"neutral good,chaotic good,neutral,chaotic neutral,neutral evil,chaotic evil\"\n",
        "                #         elif \"evil\" in each:\n",
        "                #             val = \"lawful evil,neutral evil,chaotic evil\"\n",
        "                #         elif \"chaotic\" in each:\n",
        "                #             val = \"chaotic good,chaotic neutral,chaotic evil\"\n",
        "                #         else:\n",
        "                #             val = \"lawful good,neutral good,chaotic good,lawful neutral,neutral,chaotic neutral,lawful evil,neutral evil,chaotic evil\"\n",
        "                #     elif \"or\" in each:\n",
        "                #         if \"neutral good\" in each and \"neutral evil\" in each:\n",
        "                #             val = \"neutral good,neutral evil\"\n",
        "                #         elif \"chaotic good\" in each and \"neutral evil\" in each:\n",
        "                #             val = \"chaotic good,neutral evil\"\n",
        "                #     df_copy.at[i,col] = val\n",
        "                pass\n",
        "\n",
        "\n",
        "        unique = df_copy[col].unique()\n",
        "        # if col == 'alignment': print(unique)\n",
        "        self.__update_ocv__(df_copy, col, unique)\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __reclassify_list__(self, col, delimiter):\n",
        "        df_copy = self.df.copy()\n",
        "        column = df_copy[col]\n",
        "        for i in range(0,len(column)):\n",
        "            num = 0\n",
        "            item = column[i]\n",
        "            vals = item.split(delimiter)\n",
        "            for each in vals:\n",
        "                each = each.lower()\n",
        "                if \"two\" in each: num = num + 2\n",
        "                elif \"three\" in each: num = num + 3\n",
        "                elif \"four\" in each: num = num + 4\n",
        "                elif \"five\" in each: num = num + 5\n",
        "                else: num = num + 1\n",
        "            df_copy.at[i,col] = num\n",
        "        self.df = df_copy\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    \n",
        "    # def normalize_zscore(self, col):\n",
        "    #     df_copy = self.df.copy()\n",
        "    #     df_copy[col] = zscore(df_copy[col])\n",
        "    #     self.df = df_copy\n",
        "    \n",
        "    def getocv(self):\n",
        "        return self.original_categorical_vals\n",
        "    \n",
        "    def create_subdf(self,substring):\n",
        "        df_copy = self.df.copy()\n",
        "        subdf = pd.DataFrame()\n",
        "        for each in df_copy:\n",
        "            if substring in each:\n",
        "                subdf = pd.concat([subdf,df_copy[each]],axis=1)\n",
        "        return subdf\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        df_copy = self.df.copy()\n",
        "        \n",
        "        monster_name = df_copy['name']\n",
        "        cat_size = self.create_subdf(\"size\")\n",
        "        cat_type = self.create_subdf(\"type\")\n",
        "        cat_alignment = self.create_subdf(\"alignment\")\n",
        "        cat_legendary = self.create_subdf(\"legendary\")\n",
        "        numeric = df_copy.copy()\n",
        "        numeric = numeric.drop(cat_size.columns,axis=1)\n",
        "        numeric = numeric.drop(cat_type.columns,axis=1)\n",
        "        numeric = numeric.drop(cat_alignment.columns,axis=1)\n",
        "        numeric = numeric.drop(cat_legendary.columns,axis=1)\n",
        "        numeric = numeric.drop(['name','cr'],axis=1)\n",
        "        target_value = df_copy['cr']\n",
        "\n",
        "        monster_name = monster_name[idx]\n",
        "        cat_size = cat_size.iloc[idx]\n",
        "        cat_type = cat_type.iloc[idx]\n",
        "        cat_alignment = cat_alignment.iloc[idx]\n",
        "        cat_legendary = cat_legendary.iloc[idx]\n",
        "        numeric = numeric.iloc[idx]\n",
        "        target_value = target_value[idx]\n",
        "\n",
        "        # cat_size = torch.tensor(cat_size, dtype=torch.long)\n",
        "        # cat_type = torch.tensor(cat_type, dtype=torch.long)\n",
        "        # cat_alignment = torch.tensor(cat_alignment, dtype=torch.long)\n",
        "        # cat_legendary = torch.tensor(cat_legendary, dtype=torch.long)\n",
        "        # why are we turning things into longs? should they just be floats?\n",
        "        cat_size = torch.tensor(cat_size, dtype=torch.float32)\n",
        "        cat_type = torch.tensor(cat_type, dtype=torch.float32)\n",
        "        cat_alignment = torch.tensor(cat_alignment, dtype=torch.float32)\n",
        "        cat_legendary = torch.tensor(cat_legendary, dtype=torch.float32)\n",
        "        numeric = torch.tensor(numeric, dtype=torch.float32)\n",
        "        target_value = torch.tensor(int(target_value), dtype=torch.long)\n",
        "\n",
        "        return monster_name,numeric,cat_size,cat_type,cat_alignment,cat_legendary,target_value\n",
        "    \n",
        "    def getdf(self):\n",
        "        return self.df\n",
        "\n",
        "train_dataset = MonsterDataset(\"aidedd_blocks2.csv\",\"\")\n",
        "test_dataset = MonsterDataset(\"aidedd_blocks2.csv\",\"\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "train_df = train_dataset.getdf()\n",
        "\n",
        "input_size = len(train_df.columns) # 85 including all the expanded categorical data\n",
        "\n",
        "# train_dataset.__getitem__(2)\n",
        "# print(train_dataset.__getitem__(2))\n",
        "\n",
        "ocv = train_dataset.getocv()\n",
        "for each in ocv:\n",
        "    print(each, len(ocv[each].dropna()))\n",
        "print(\"input size\",input_size)\n",
        "# ocv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 396,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6oUMaolB8ge",
        "outputId": "45483e63-8ca6-4d51-ebc3-55043de025c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, None) torch.Size([100, 83])\n",
            "(None,) torch.Size([100])\n",
            "(None, None) torch.Size([33, 100])\n",
            "(None,) torch.Size([33])\n"
          ]
        }
      ],
      "source": [
        "class CRPredictor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(CRPredictor,self).__init__()# Process numeric features\n",
        "        # self.emb_size = nn.Embedding(7,2)\n",
        "        # self.emb_alignment = nn.Embedding(18,2)\n",
        "        # self.emb_type = nn.Embedding(16,2)\n",
        "        # self.emb_legendary = nn.Embedding(2,2)\n",
        "        # self.n_emb = 7+18+16+2 # total = 43\n",
        "        \n",
        "        self.l1 = nn.Linear(83,hidden_size) # first layer\n",
        "        self.relu = nn.ReLU() # activation function\n",
        "        self.l2 = nn.Linear(hidden_size,num_classes) # second layer\n",
        "        self.softmax = nn.Softmax()\n",
        "    \n",
        "    def forward(self, numeric, cat1,cat2,cat3,cat4):\n",
        "        # emb1 = self.emb_size(cat1)\n",
        "        # emb2 = self.emb_alignment(cat2)\n",
        "        # emb3 = self.emb_type(cat3)\n",
        "        # emb4 = self.emb_legendary(cat4)\n",
        "\n",
        "        x = torch.cat([numeric, cat1,cat2,cat3,cat4],dim=1)\n",
        "        # print(\"em1size\",x_emb.size())\n",
        "        # print(x_emb)\n",
        "        # print(\"em1size\",emb1.size())\n",
        "        # print(\"em2size\",emb2.size())\n",
        "        # print(\"em3size\",emb3.size())\n",
        "        # print(\"em4size\",emb4.size())\n",
        "        # print(\"numsize\",numerical.size())\n",
        "        \n",
        "\n",
        "        # x = torch.cat([x_emb,numerical],dim=1)\n",
        "        x = self.l1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.l2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "model = CRPredictor(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "for param in model.parameters():\n",
        "    print(param.names, param.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 397,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "WWfsBaoGhNoz",
        "outputId": "16cf988b-4935-41ed-b44c-90df8cfe0eac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8\n",
            "epoch 1/2, step 4/8, loss = 3.4388\n",
            "epoch 1/2, step 8/8, loss = 3.2980\n",
            "epoch 2/2, step 4/8, loss = 3.3229\n",
            "epoch 2/2, step 8/8, loss = 3.3187\n",
            "Finished training.\n"
          ]
        }
      ],
      "source": [
        "n_total_steps = len(train_loader)\n",
        "print(n_total_steps)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (name,numeric,cat1,cat2,cat3,cat4,target) in enumerate(train_loader):\n",
        "        \n",
        "        outputs = model(numeric,cat1,cat2,cat3,cat4)\n",
        "        loss = criterion(outputs,target)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 4 == 0:\n",
        "            print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')\n",
        "            # print(f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_total_steps}, loss = {0:.4f}')\n",
        "\n",
        "print(\"Finished training.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSZul2aPBMeB"
      },
      "source": [
        "# Test Code Clipboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnk91r0x-uPq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aienv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
